{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 构建网络\n",
    "\n",
    "论文名称：《Event-based High Dynamic Range Image and Very High Frame Rate Video Generation using Conditional Generative Adversarial Networks》"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 生成模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class HDRGen(nn.Module):\n",
    "    def __init__(self,dropout = 0.5):\n",
    "        super(HDRGen,self).__init__()\n",
    "        self.Encoder1 = nn.Sequential(\n",
    "            # input(3，256，256)\n",
    "            nn.Conv2d(3,64,3,stride=2,padding=1),\n",
    "            nn.InstanceNorm2d(3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.Encoder2=nn.Sequential(\n",
    "            #input(64,128,128)\n",
    "            nn.Conv2d(64,128,3,stride=2,padding=1),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.Encoder3 = nn.Sequential(\n",
    "            #input(128,64,64)\n",
    "            nn.Conv2d(128,256,3,stride=2,padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.Encoder4 = nn.Sequential(\n",
    "            #input(256,32,32)\n",
    "            nn.Conv2d(256,512,3,stride=2,padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.Encoder5=nn.Sequential(\n",
    "            #input(512,16,16)\n",
    "            nn.Conv2d(512,512,3,stride=2,padding=1),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.Encoder6=nn.Sequential(\n",
    "            #input(512,8,8)\n",
    "            nn.Conv2d(512,512,3,stride=2,padding=1),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.Encoder7=nn.Sequential(\n",
    "            #Input(512,4,4)\n",
    "            nn.Conv2d(512,512,3,stride=2,padding=1),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.Encoder8=nn.Sequential(\n",
    "            #input(512,2,2)\n",
    "            nn.Conv2d(512,512,2,stride=2),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.Decoder8 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512,512,2,stride=2),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # 这里第一次使用反卷积，注意，从低纬向高纬进行反卷积往往不是一一对应的，为了使反卷积和卷积对应起来，通常来讲,padding = (ksize-1)/2 ,output_padding=stride-1\n",
    "        self.Decoder7 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512,512,3,stride=2,padding=1,output_padding=1),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.Decoder6 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512,512,3,stride=2,padding=1,output_padding=1),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.Decoder5=nn.Sequential(\n",
    "            nn.ConvTranspose2d(512,512,3,stride=2,padding=1,output_padding=1),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.Decoder4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512,256,3,stride=2,padding=1,output_padding=1),\n",
    "            nn.InstanceNorm2d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.Decoder3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256,128,3,stride=2,padding=1,output_padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.Decoder2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128,64,3,stride=2,padding=1,output_padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.Decoder1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64,1,3,stride=2,padding=1,output_padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self,frame):\n",
    "        #input(N,3,256,256)\n",
    "        encoder1 = self.Encoder1(frame)\n",
    "        encoder2 = self.Encoder2(encoder1)\n",
    "        encoder3 = self.Encoder3(encoder2)\n",
    "        encoder4 = self.Encoder4(encoder3)\n",
    "        encoder5 = self.Encoder5(encoder4)\n",
    "        encoder6 = self.Encoder6(encoder5)\n",
    "        encoder7 = self.Encoder7(encoder6)\n",
    "        encoder8 = self.Encoder8(encoder7)\n",
    "        \n",
    "        decoder8 = self.Decoder8(encoder8)+encoder7\n",
    "        decoder7 = self.Decoder7(decoder8)+encoder6\n",
    "        decoder6 = self.Decoder6(decoder7)+encoder5\n",
    "        decoder5 = self.Decoder5(decoder6)+encoder4\n",
    "        decoder4 = self.Decoder4(decoder5)+encoder3\n",
    "        decoder3 = self.Decoder3(decoder4)+encoder2\n",
    "        decoder2 = self.Decoder2(decoder3)+encoder1\n",
    "        output = self.Decoder1(decoder2)\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2判别模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDRDis(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HDRDis,self).__init__()\n",
    "        self.Discrim = nn.Sequential(\n",
    "            # input(4,256,256)\n",
    "            nn.Conv2d(4,64,3,stride=2,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64,128,3,stride=2,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128,256,3,stride=2,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256,512,3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512,1,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self,Events, Gimage):\n",
    "        frame = torch.cat((Events,Gimage),dim=1)\n",
    "        # input(N,4,256,256)\n",
    "        output = self.Discrim(frame)\n",
    "        return torch.mean(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(G_loss,self).__init__()\n",
    "    def forward(self,G,e,g):\n",
    "        return -torch.mean(torch.log(D(e,G(e))))+torch.mean(torch.abs(g-G(e)))\n",
    "        \n",
    "class D_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(D_loss,self).__init__()\n",
    "    def forward(self,D,G,e,g):\n",
    "        return -torch.log(D(e,g))-torch.log(1-D(e,G(e)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 256, 256]) torch.Size([1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "class Datain(Dataset):\n",
    "    def __init__(self,InputPath):\n",
    "        self.InputPath = InputPath\n",
    "        self.FileList=os.listdir(self.InputPath)\n",
    "        self.Length = len(self.FileList)\n",
    "        \n",
    "    def Float2Image(self,InputFloat):\n",
    "        Output = 127.5*InputFloat+127.5\n",
    "        return Output.astype(int)\n",
    "\n",
    "    def Image2Float(self,InputImage):\n",
    "        InputImage = InputImage.astype(float)\n",
    "        return 2*InputImage/255 -1\n",
    "    \n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        Image = cv2.imread(os.path.join(self.InputPath,self.FileList[index]))\n",
    "        Image = cv2.resize(Image,(512,256),interpolation=cv2.INTER_CUBIC)\n",
    "        Image = self.Image2Float(Image)\n",
    "        Image = torch.tensor(Image,dtype = torch.float32)\n",
    "        Input,Output = Image.chunk(2,1)\n",
    "        Output,_,_ = Output.chunk(3,2)\n",
    "        return Input.permute(2,0,1),Output.permute(2,0,1)\n",
    "    def __len__(self):\n",
    "        return self.Length\n",
    "    \n",
    "TrainData = Datain('Sim_EBS_train')\n",
    "print(TrainData[0][0].shape,TrainData[0][1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 实验部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G model load successfully\n",
      "D model load successfully\n",
      "Batch:0/88 BatchLoss: G:2.175,D:0.670\n",
      "Batch:1/88 BatchLoss: G:1.917,D:0.809\n",
      "Batch:2/88 BatchLoss: G:1.766,D:1.032\n",
      "Batch:3/88 BatchLoss: G:1.669,D:1.275\n",
      "Batch:4/88 BatchLoss: G:1.553,D:1.320\n",
      "Batch:5/88 BatchLoss: G:1.542,D:1.253\n",
      "Batch:6/88 BatchLoss: G:1.605,D:1.319\n",
      "Batch:7/88 BatchLoss: G:1.567,D:1.381\n",
      "Batch:8/88 BatchLoss: G:1.724,D:1.356\n",
      "Batch:9/88 BatchLoss: G:1.681,D:1.405\n",
      "Batch:10/88 BatchLoss: G:1.736,D:1.238\n",
      "Batch:11/88 BatchLoss: G:1.644,D:1.234\n",
      "Batch:12/88 BatchLoss: G:1.745,D:1.057\n",
      "Batch:13/88 BatchLoss: G:1.739,D:1.034\n",
      "Batch:14/88 BatchLoss: G:1.748,D:0.930\n",
      "Batch:15/88 BatchLoss: G:1.888,D:0.869\n",
      "Batch:16/88 BatchLoss: G:2.326,D:0.785\n",
      "Batch:17/88 BatchLoss: G:3.325,D:0.705\n",
      "Batch:18/88 BatchLoss: G:3.476,D:0.648\n",
      "Batch:19/88 BatchLoss: G:3.095,D:0.574\n",
      "Batch:20/88 BatchLoss: G:2.778,D:0.479\n",
      "Batch:21/88 BatchLoss: G:2.580,D:0.433\n",
      "Batch:22/88 BatchLoss: G:2.427,D:0.396\n",
      "Batch:23/88 BatchLoss: G:2.502,D:0.360\n",
      "Batch:24/88 BatchLoss: G:2.568,D:0.377\n",
      "Batch:25/88 BatchLoss: G:2.735,D:0.333\n",
      "Batch:26/88 BatchLoss: G:2.869,D:0.311\n",
      "Batch:27/88 BatchLoss: G:3.131,D:0.261\n",
      "Batch:28/88 BatchLoss: G:3.365,D:0.224\n",
      "Batch:29/88 BatchLoss: G:3.739,D:0.204\n",
      "Batch:30/88 BatchLoss: G:3.852,D:0.216\n",
      "Batch:31/88 BatchLoss: G:4.084,D:0.179\n",
      "Batch:32/88 BatchLoss: G:4.348,D:0.185\n",
      "Batch:33/88 BatchLoss: G:3.913,D:0.179\n",
      "Batch:34/88 BatchLoss: G:3.767,D:0.224\n",
      "Batch:35/88 BatchLoss: G:2.742,D:0.286\n",
      "Batch:36/88 BatchLoss: G:2.251,D:0.421\n",
      "Batch:37/88 BatchLoss: G:2.059,D:0.579\n",
      "Batch:38/88 BatchLoss: G:1.956,D:0.615\n",
      "Batch:39/88 BatchLoss: G:2.070,D:0.638\n",
      "Batch:40/88 BatchLoss: G:2.130,D:0.596\n",
      "Batch:41/88 BatchLoss: G:2.287,D:0.500\n",
      "Batch:42/88 BatchLoss: G:2.399,D:0.507\n",
      "Batch:43/88 BatchLoss: G:2.377,D:0.521\n",
      "Batch:44/88 BatchLoss: G:1.795,D:0.636\n",
      "Batch:45/88 BatchLoss: G:1.846,D:0.472\n",
      "Batch:46/88 BatchLoss: G:1.919,D:0.476\n",
      "Batch:47/88 BatchLoss: G:2.102,D:0.494\n",
      "Batch:48/88 BatchLoss: G:2.468,D:0.445\n",
      "Batch:49/88 BatchLoss: G:3.378,D:0.347\n",
      "Batch:50/88 BatchLoss: G:4.036,D:0.304\n",
      "Batch:51/88 BatchLoss: G:4.569,D:0.354\n",
      "Batch:52/88 BatchLoss: G:4.055,D:0.416\n",
      "Batch:53/88 BatchLoss: G:3.476,D:0.314\n",
      "Batch:54/88 BatchLoss: G:2.775,D:0.299\n",
      "Batch:55/88 BatchLoss: G:2.398,D:0.324\n",
      "Batch:56/88 BatchLoss: G:2.572,D:0.326\n",
      "Batch:57/88 BatchLoss: G:2.518,D:0.404\n",
      "Batch:58/88 BatchLoss: G:2.974,D:0.401\n",
      "Batch:59/88 BatchLoss: G:3.937,D:0.387\n",
      "Batch:60/88 BatchLoss: G:4.565,D:0.384\n",
      "Batch:61/88 BatchLoss: G:3.720,D:0.488\n",
      "Batch:62/88 BatchLoss: G:2.678,D:0.435\n",
      "Batch:63/88 BatchLoss: G:2.248,D:0.455\n",
      "Batch:64/88 BatchLoss: G:2.440,D:0.512\n",
      "Batch:65/88 BatchLoss: G:3.219,D:0.470\n",
      "Batch:66/88 BatchLoss: G:3.019,D:0.555\n",
      "Batch:67/88 BatchLoss: G:2.389,D:0.535\n",
      "Batch:68/88 BatchLoss: G:2.442,D:0.483\n",
      "Batch:69/88 BatchLoss: G:3.047,D:0.476\n",
      "Batch:70/88 BatchLoss: G:3.317,D:0.482\n",
      "Batch:71/88 BatchLoss: G:2.933,D:0.543\n",
      "Batch:72/88 BatchLoss: G:2.364,D:0.475\n",
      "Batch:73/88 BatchLoss: G:2.143,D:0.453\n",
      "Batch:74/88 BatchLoss: G:2.216,D:0.514\n",
      "Batch:75/88 BatchLoss: G:2.404,D:0.517\n",
      "Batch:76/88 BatchLoss: G:2.379,D:0.569\n",
      "Batch:77/88 BatchLoss: G:2.175,D:0.592\n",
      "Batch:78/88 BatchLoss: G:1.843,D:0.604\n",
      "Batch:79/88 BatchLoss: G:1.748,D:0.607\n",
      "Batch:80/88 BatchLoss: G:1.835,D:0.683\n",
      "Batch:81/88 BatchLoss: G:1.860,D:0.669\n",
      "Batch:82/88 BatchLoss: G:1.783,D:0.685\n",
      "Batch:83/88 BatchLoss: G:1.655,D:0.660\n",
      "Batch:84/88 BatchLoss: G:1.649,D:0.674\n",
      "Batch:85/88 BatchLoss: G:1.731,D:0.706\n",
      "Batch:86/88 BatchLoss: G:1.931,D:0.657\n",
      "Batch:87/88 BatchLoss: G:2.004,D:0.656\n",
      "EpochLoss:G:223.4109344482422,D:51.85417938232422\n",
      "Batch:0/88 BatchLoss: G:1.958,D:0.597\n",
      "Batch:1/88 BatchLoss: G:1.960,D:0.535\n",
      "Batch:2/88 BatchLoss: G:2.067,D:0.518\n",
      "Batch:3/88 BatchLoss: G:2.446,D:0.505\n",
      "Batch:4/88 BatchLoss: G:2.981,D:0.474\n",
      "Batch:5/88 BatchLoss: G:2.550,D:0.493\n",
      "Batch:6/88 BatchLoss: G:2.190,D:0.544\n",
      "Batch:7/88 BatchLoss: G:1.939,D:0.565\n",
      "Batch:8/88 BatchLoss: G:1.978,D:0.491\n",
      "Batch:9/88 BatchLoss: G:2.147,D:0.436\n",
      "Batch:10/88 BatchLoss: G:2.133,D:0.452\n",
      "Batch:11/88 BatchLoss: G:2.226,D:0.454\n",
      "Batch:12/88 BatchLoss: G:2.055,D:0.508\n",
      "Batch:13/88 BatchLoss: G:2.017,D:0.519\n",
      "Batch:14/88 BatchLoss: G:2.052,D:0.525\n",
      "Batch:15/88 BatchLoss: G:2.052,D:0.574\n",
      "Batch:16/88 BatchLoss: G:1.993,D:0.613\n",
      "Batch:17/88 BatchLoss: G:2.325,D:0.509\n",
      "Batch:18/88 BatchLoss: G:2.179,D:0.536\n",
      "Batch:19/88 BatchLoss: G:2.138,D:0.513\n",
      "Batch:20/88 BatchLoss: G:2.108,D:0.512\n",
      "Batch:21/88 BatchLoss: G:2.484,D:0.520\n",
      "Batch:22/88 BatchLoss: G:2.734,D:0.494\n",
      "Batch:23/88 BatchLoss: G:2.906,D:0.460\n",
      "Batch:24/88 BatchLoss: G:2.893,D:0.502\n",
      "Batch:25/88 BatchLoss: G:2.564,D:0.495\n",
      "Batch:26/88 BatchLoss: G:2.679,D:0.393\n",
      "Batch:27/88 BatchLoss: G:2.367,D:0.476\n",
      "Batch:28/88 BatchLoss: G:2.466,D:0.465\n",
      "Batch:29/88 BatchLoss: G:2.566,D:0.491\n",
      "Batch:30/88 BatchLoss: G:2.457,D:0.506\n",
      "Batch:31/88 BatchLoss: G:2.020,D:0.577\n",
      "Batch:32/88 BatchLoss: G:2.079,D:0.553\n",
      "Batch:33/88 BatchLoss: G:1.856,D:0.491\n",
      "Batch:34/88 BatchLoss: G:1.888,D:0.494\n",
      "Batch:35/88 BatchLoss: G:2.052,D:0.459\n",
      "Batch:36/88 BatchLoss: G:2.034,D:0.535\n",
      "Batch:37/88 BatchLoss: G:2.056,D:0.494\n",
      "Batch:38/88 BatchLoss: G:2.128,D:0.498\n",
      "Batch:39/88 BatchLoss: G:2.323,D:0.444\n",
      "Batch:40/88 BatchLoss: G:2.451,D:0.362\n",
      "Batch:41/88 BatchLoss: G:2.361,D:0.441\n",
      "Batch:42/88 BatchLoss: G:2.554,D:0.422\n",
      "Batch:43/88 BatchLoss: G:3.036,D:0.334\n",
      "Batch:44/88 BatchLoss: G:2.252,D:0.380\n",
      "Batch:45/88 BatchLoss: G:1.975,D:0.421\n",
      "Batch:46/88 BatchLoss: G:2.768,D:0.414\n",
      "Batch:47/88 BatchLoss: G:3.135,D:0.332\n",
      "Batch:48/88 BatchLoss: G:2.666,D:0.463\n",
      "Batch:49/88 BatchLoss: G:2.160,D:0.403\n",
      "Batch:50/88 BatchLoss: G:1.916,D:0.522\n",
      "Batch:51/88 BatchLoss: G:2.253,D:0.440\n",
      "Batch:52/88 BatchLoss: G:2.362,D:0.470\n",
      "Batch:53/88 BatchLoss: G:2.205,D:0.588\n",
      "Batch:54/88 BatchLoss: G:1.954,D:0.494\n",
      "Batch:55/88 BatchLoss: G:1.835,D:0.601\n",
      "Batch:56/88 BatchLoss: G:2.002,D:0.590\n",
      "Batch:57/88 BatchLoss: G:2.298,D:0.565\n",
      "Batch:58/88 BatchLoss: G:2.360,D:0.510\n",
      "Batch:59/88 BatchLoss: G:2.004,D:0.591\n",
      "Batch:60/88 BatchLoss: G:2.134,D:0.557\n",
      "Batch:61/88 BatchLoss: G:2.196,D:0.515\n",
      "Batch:62/88 BatchLoss: G:2.338,D:0.552\n",
      "Batch:63/88 BatchLoss: G:2.579,D:0.577\n",
      "Batch:64/88 BatchLoss: G:2.606,D:0.602\n",
      "Batch:65/88 BatchLoss: G:2.006,D:0.555\n",
      "Batch:66/88 BatchLoss: G:1.849,D:0.482\n",
      "Batch:67/88 BatchLoss: G:1.997,D:0.559\n",
      "Batch:68/88 BatchLoss: G:2.223,D:0.498\n",
      "Batch:69/88 BatchLoss: G:2.072,D:0.524\n",
      "Batch:70/88 BatchLoss: G:2.040,D:0.521\n",
      "Batch:71/88 BatchLoss: G:2.988,D:0.410\n",
      "Batch:72/88 BatchLoss: G:3.238,D:0.425\n",
      "Batch:73/88 BatchLoss: G:3.034,D:0.461\n",
      "Batch:74/88 BatchLoss: G:2.623,D:0.532\n",
      "Batch:75/88 BatchLoss: G:1.688,D:0.477\n",
      "Batch:76/88 BatchLoss: G:2.111,D:0.497\n",
      "Batch:77/88 BatchLoss: G:3.990,D:0.440\n",
      "Batch:78/88 BatchLoss: G:3.293,D:0.573\n",
      "Batch:79/88 BatchLoss: G:1.725,D:0.398\n",
      "Batch:80/88 BatchLoss: G:2.288,D:0.554\n",
      "Batch:81/88 BatchLoss: G:3.037,D:0.390\n",
      "Batch:82/88 BatchLoss: G:3.913,D:0.433\n",
      "Batch:83/88 BatchLoss: G:3.469,D:0.480\n",
      "Batch:84/88 BatchLoss: G:2.664,D:0.419\n",
      "Batch:85/88 BatchLoss: G:2.575,D:0.433\n",
      "Batch:86/88 BatchLoss: G:3.382,D:0.445\n",
      "Batch:87/88 BatchLoss: G:3.615,D:0.327\n",
      "EpochLoss:G:210.26560974121094,D:43.19749450683594\n",
      "Batch:0/88 BatchLoss: G:2.247,D:0.410\n",
      "Batch:1/88 BatchLoss: G:2.810,D:0.467\n",
      "Batch:2/88 BatchLoss: G:3.423,D:0.438\n",
      "Batch:3/88 BatchLoss: G:2.645,D:0.493\n",
      "Batch:4/88 BatchLoss: G:2.341,D:0.492\n",
      "Batch:5/88 BatchLoss: G:2.877,D:0.443\n",
      "Batch:6/88 BatchLoss: G:3.242,D:0.445\n",
      "Batch:7/88 BatchLoss: G:3.076,D:0.514\n",
      "Batch:8/88 BatchLoss: G:2.497,D:0.452\n",
      "Batch:9/88 BatchLoss: G:2.136,D:0.426\n",
      "Batch:10/88 BatchLoss: G:2.636,D:0.501\n",
      "Batch:11/88 BatchLoss: G:2.907,D:0.429\n",
      "Batch:12/88 BatchLoss: G:2.584,D:0.532\n",
      "Batch:13/88 BatchLoss: G:2.275,D:0.569\n",
      "Batch:14/88 BatchLoss: G:2.050,D:0.535\n",
      "Batch:15/88 BatchLoss: G:1.573,D:0.513\n",
      "Batch:16/88 BatchLoss: G:1.901,D:0.635\n",
      "Batch:17/88 BatchLoss: G:2.915,D:0.519\n",
      "Batch:18/88 BatchLoss: G:2.870,D:0.497\n",
      "Batch:19/88 BatchLoss: G:2.668,D:0.650\n",
      "Batch:20/88 BatchLoss: G:2.172,D:0.666\n",
      "Batch:21/88 BatchLoss: G:1.810,D:0.619\n",
      "Batch:22/88 BatchLoss: G:1.625,D:0.511\n",
      "Batch:23/88 BatchLoss: G:1.615,D:0.613\n",
      "Batch:24/88 BatchLoss: G:2.051,D:0.479\n",
      "Batch:25/88 BatchLoss: G:3.512,D:0.377\n",
      "Batch:26/88 BatchLoss: G:3.552,D:0.404\n",
      "Batch:27/88 BatchLoss: G:3.135,D:0.362\n",
      "Batch:28/88 BatchLoss: G:2.953,D:0.518\n",
      "Batch:29/88 BatchLoss: G:2.643,D:0.592\n",
      "Batch:30/88 BatchLoss: G:2.568,D:0.705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:31/88 BatchLoss: G:2.569,D:0.519\n",
      "Batch:32/88 BatchLoss: G:2.503,D:0.623\n",
      "Batch:33/88 BatchLoss: G:2.731,D:0.413\n",
      "Batch:34/88 BatchLoss: G:2.444,D:0.534\n",
      "Batch:35/88 BatchLoss: G:1.932,D:0.468\n",
      "Batch:36/88 BatchLoss: G:2.341,D:0.434\n",
      "Batch:37/88 BatchLoss: G:3.062,D:0.437\n",
      "Batch:38/88 BatchLoss: G:3.358,D:0.506\n",
      "Batch:39/88 BatchLoss: G:2.558,D:0.442\n",
      "Batch:40/88 BatchLoss: G:2.414,D:0.513\n",
      "Batch:41/88 BatchLoss: G:2.554,D:0.600\n",
      "Batch:42/88 BatchLoss: G:2.927,D:0.519\n",
      "Batch:43/88 BatchLoss: G:2.160,D:0.440\n",
      "Batch:44/88 BatchLoss: G:2.143,D:0.452\n",
      "Batch:45/88 BatchLoss: G:2.645,D:0.418\n",
      "Batch:46/88 BatchLoss: G:2.792,D:0.589\n",
      "Batch:47/88 BatchLoss: G:3.055,D:0.574\n",
      "Batch:48/88 BatchLoss: G:3.060,D:0.590\n",
      "Batch:49/88 BatchLoss: G:2.916,D:0.530\n",
      "Batch:50/88 BatchLoss: G:2.617,D:0.502\n",
      "Batch:51/88 BatchLoss: G:1.820,D:0.587\n",
      "Batch:52/88 BatchLoss: G:1.891,D:0.524\n",
      "Batch:53/88 BatchLoss: G:2.260,D:0.463\n",
      "Batch:54/88 BatchLoss: G:2.438,D:0.528\n",
      "Batch:55/88 BatchLoss: G:2.593,D:0.504\n",
      "Batch:56/88 BatchLoss: G:1.900,D:0.552\n",
      "Batch:57/88 BatchLoss: G:1.645,D:0.528\n",
      "Batch:58/88 BatchLoss: G:2.309,D:0.570\n",
      "Batch:59/88 BatchLoss: G:2.655,D:0.421\n",
      "Batch:60/88 BatchLoss: G:2.561,D:0.523\n",
      "Batch:61/88 BatchLoss: G:2.758,D:0.476\n",
      "Batch:62/88 BatchLoss: G:2.642,D:0.433\n",
      "Batch:63/88 BatchLoss: G:2.618,D:0.366\n",
      "Batch:64/88 BatchLoss: G:2.347,D:0.407\n",
      "Batch:65/88 BatchLoss: G:1.556,D:0.534\n",
      "Batch:66/88 BatchLoss: G:1.775,D:0.532\n",
      "Batch:67/88 BatchLoss: G:1.860,D:0.608\n",
      "Batch:68/88 BatchLoss: G:2.501,D:0.619\n",
      "Batch:69/88 BatchLoss: G:1.932,D:0.602\n",
      "Batch:70/88 BatchLoss: G:1.652,D:0.521\n",
      "Batch:71/88 BatchLoss: G:1.765,D:0.603\n",
      "Batch:72/88 BatchLoss: G:2.255,D:0.522\n",
      "Batch:73/88 BatchLoss: G:2.167,D:0.687\n",
      "Batch:74/88 BatchLoss: G:2.247,D:0.576\n",
      "Batch:75/88 BatchLoss: G:2.147,D:0.684\n",
      "Batch:76/88 BatchLoss: G:1.965,D:0.680\n",
      "Batch:77/88 BatchLoss: G:1.982,D:0.714\n",
      "Batch:78/88 BatchLoss: G:2.191,D:0.533\n",
      "Batch:79/88 BatchLoss: G:1.991,D:0.748\n",
      "Batch:80/88 BatchLoss: G:1.697,D:0.708\n",
      "Batch:81/88 BatchLoss: G:1.659,D:0.557\n",
      "Batch:82/88 BatchLoss: G:1.643,D:0.647\n",
      "Batch:83/88 BatchLoss: G:1.771,D:0.626\n",
      "Batch:84/88 BatchLoss: G:1.852,D:0.611\n",
      "Batch:85/88 BatchLoss: G:1.783,D:0.650\n",
      "Batch:86/88 BatchLoss: G:1.673,D:0.705\n",
      "Batch:87/88 BatchLoss: G:1.521,D:0.627\n",
      "EpochLoss:G:208.10888671875,D:47.082881927490234\n",
      "Batch:0/88 BatchLoss: G:1.589,D:0.559\n",
      "Batch:1/88 BatchLoss: G:1.880,D:0.470\n",
      "Batch:2/88 BatchLoss: G:2.109,D:0.566\n",
      "Batch:3/88 BatchLoss: G:1.680,D:0.675\n",
      "Batch:4/88 BatchLoss: G:1.720,D:0.456\n",
      "Batch:5/88 BatchLoss: G:1.651,D:0.534\n",
      "Batch:6/88 BatchLoss: G:1.772,D:0.529\n",
      "Batch:7/88 BatchLoss: G:1.760,D:0.568\n",
      "Batch:8/88 BatchLoss: G:1.876,D:0.520\n",
      "Batch:9/88 BatchLoss: G:1.709,D:0.540\n",
      "Batch:10/88 BatchLoss: G:1.532,D:0.654\n",
      "Batch:11/88 BatchLoss: G:1.689,D:0.612\n",
      "Batch:12/88 BatchLoss: G:1.862,D:0.543\n",
      "Batch:13/88 BatchLoss: G:1.774,D:0.579\n",
      "Batch:14/88 BatchLoss: G:1.641,D:0.612\n",
      "Batch:15/88 BatchLoss: G:1.896,D:0.581\n",
      "Batch:16/88 BatchLoss: G:2.125,D:0.589\n",
      "Batch:17/88 BatchLoss: G:2.066,D:0.633\n",
      "Batch:18/88 BatchLoss: G:2.012,D:0.634\n",
      "Batch:19/88 BatchLoss: G:2.049,D:0.541\n",
      "Batch:20/88 BatchLoss: G:2.389,D:0.510\n",
      "Batch:21/88 BatchLoss: G:2.658,D:0.434\n",
      "Batch:22/88 BatchLoss: G:1.881,D:0.427\n",
      "Batch:23/88 BatchLoss: G:2.038,D:0.556\n",
      "Batch:24/88 BatchLoss: G:2.968,D:0.463\n",
      "Batch:25/88 BatchLoss: G:2.225,D:0.510\n",
      "Batch:26/88 BatchLoss: G:1.734,D:0.455\n",
      "Batch:27/88 BatchLoss: G:2.635,D:0.504\n",
      "Batch:28/88 BatchLoss: G:2.848,D:0.517\n",
      "Batch:29/88 BatchLoss: G:2.705,D:0.552\n",
      "Batch:30/88 BatchLoss: G:2.223,D:0.550\n",
      "Batch:31/88 BatchLoss: G:1.913,D:0.530\n",
      "Batch:32/88 BatchLoss: G:2.000,D:0.621\n",
      "Batch:33/88 BatchLoss: G:2.289,D:0.763\n",
      "Batch:34/88 BatchLoss: G:2.344,D:0.750\n",
      "Batch:35/88 BatchLoss: G:2.138,D:0.740\n",
      "Batch:36/88 BatchLoss: G:2.004,D:0.750\n",
      "Batch:37/88 BatchLoss: G:1.890,D:0.812\n",
      "Batch:38/88 BatchLoss: G:2.116,D:0.647\n",
      "Batch:39/88 BatchLoss: G:2.404,D:0.650\n",
      "Batch:40/88 BatchLoss: G:2.793,D:0.741\n",
      "Batch:41/88 BatchLoss: G:2.695,D:0.808\n",
      "Batch:42/88 BatchLoss: G:2.406,D:0.735\n",
      "Batch:43/88 BatchLoss: G:2.097,D:0.685\n",
      "Batch:44/88 BatchLoss: G:2.985,D:0.460\n",
      "Batch:45/88 BatchLoss: G:3.105,D:0.340\n",
      "Batch:46/88 BatchLoss: G:3.054,D:0.389\n",
      "Batch:47/88 BatchLoss: G:2.864,D:0.432\n",
      "Batch:48/88 BatchLoss: G:2.794,D:0.399\n",
      "Batch:49/88 BatchLoss: G:2.544,D:0.434\n",
      "Batch:50/88 BatchLoss: G:2.240,D:0.446\n",
      "Batch:51/88 BatchLoss: G:2.016,D:0.467\n",
      "Batch:52/88 BatchLoss: G:1.987,D:0.530\n",
      "Batch:53/88 BatchLoss: G:2.307,D:0.506\n",
      "Batch:54/88 BatchLoss: G:3.057,D:0.580\n",
      "Batch:55/88 BatchLoss: G:2.377,D:0.672\n",
      "Batch:56/88 BatchLoss: G:1.814,D:0.635\n",
      "Batch:57/88 BatchLoss: G:1.716,D:0.643\n",
      "Batch:58/88 BatchLoss: G:2.048,D:0.656\n",
      "Batch:59/88 BatchLoss: G:2.512,D:0.625\n",
      "Batch:60/88 BatchLoss: G:2.674,D:0.609\n",
      "Batch:61/88 BatchLoss: G:2.012,D:0.687\n",
      "Batch:62/88 BatchLoss: G:1.765,D:0.610\n",
      "Batch:63/88 BatchLoss: G:2.072,D:0.570\n",
      "Batch:64/88 BatchLoss: G:2.747,D:0.462\n",
      "Batch:65/88 BatchLoss: G:2.700,D:0.495\n",
      "Batch:66/88 BatchLoss: G:1.848,D:0.517\n",
      "Batch:67/88 BatchLoss: G:1.970,D:0.442\n",
      "Batch:68/88 BatchLoss: G:2.116,D:0.493\n",
      "Batch:69/88 BatchLoss: G:2.258,D:0.452\n",
      "Batch:70/88 BatchLoss: G:1.853,D:0.424\n",
      "Batch:71/88 BatchLoss: G:2.049,D:0.432\n",
      "Batch:72/88 BatchLoss: G:2.665,D:0.391\n",
      "Batch:73/88 BatchLoss: G:4.090,D:0.336\n",
      "Batch:74/88 BatchLoss: G:3.376,D:0.447\n",
      "Batch:75/88 BatchLoss: G:2.924,D:0.468\n",
      "Batch:76/88 BatchLoss: G:2.453,D:0.462\n",
      "Batch:77/88 BatchLoss: G:2.770,D:0.439\n",
      "Batch:78/88 BatchLoss: G:2.669,D:0.484\n",
      "Batch:79/88 BatchLoss: G:3.047,D:0.404\n",
      "Batch:80/88 BatchLoss: G:2.775,D:0.390\n",
      "Batch:81/88 BatchLoss: G:2.442,D:0.460\n",
      "Batch:82/88 BatchLoss: G:2.310,D:0.610\n",
      "Batch:83/88 BatchLoss: G:2.480,D:0.520\n",
      "Batch:84/88 BatchLoss: G:2.377,D:0.589\n",
      "Batch:85/88 BatchLoss: G:2.284,D:0.656\n",
      "Batch:86/88 BatchLoss: G:2.078,D:0.723\n",
      "Batch:87/88 BatchLoss: G:1.942,D:0.738\n",
      "EpochLoss:G:199.95394897460938,D:48.63198471069336\n",
      "Batch:0/88 BatchLoss: G:1.944,D:0.850\n",
      "Batch:1/88 BatchLoss: G:2.172,D:0.885\n",
      "Batch:2/88 BatchLoss: G:2.440,D:0.795\n",
      "Batch:3/88 BatchLoss: G:2.530,D:0.752\n",
      "Batch:4/88 BatchLoss: G:2.634,D:0.625\n",
      "Batch:5/88 BatchLoss: G:2.543,D:0.434\n",
      "Batch:6/88 BatchLoss: G:2.297,D:0.497\n",
      "Batch:7/88 BatchLoss: G:2.303,D:0.542\n",
      "Batch:8/88 BatchLoss: G:2.301,D:0.554\n",
      "Batch:9/88 BatchLoss: G:2.391,D:0.597\n",
      "Batch:10/88 BatchLoss: G:2.257,D:0.685\n",
      "Batch:11/88 BatchLoss: G:2.208,D:0.739\n",
      "Batch:12/88 BatchLoss: G:2.111,D:0.739\n",
      "Batch:13/88 BatchLoss: G:2.451,D:0.712\n",
      "Batch:14/88 BatchLoss: G:2.167,D:0.613\n",
      "Batch:15/88 BatchLoss: G:1.711,D:0.537\n",
      "Batch:16/88 BatchLoss: G:1.978,D:0.622\n",
      "Batch:17/88 BatchLoss: G:2.285,D:0.689\n",
      "Batch:18/88 BatchLoss: G:2.467,D:0.602\n",
      "Batch:19/88 BatchLoss: G:2.487,D:0.594\n",
      "Batch:20/88 BatchLoss: G:2.755,D:0.683\n",
      "Batch:21/88 BatchLoss: G:2.818,D:0.649\n",
      "Batch:22/88 BatchLoss: G:2.699,D:0.583\n",
      "Batch:23/88 BatchLoss: G:1.718,D:0.425\n",
      "Batch:24/88 BatchLoss: G:1.636,D:0.586\n",
      "Batch:25/88 BatchLoss: G:1.917,D:0.567\n",
      "Batch:26/88 BatchLoss: G:2.043,D:0.582\n",
      "Batch:27/88 BatchLoss: G:2.251,D:0.622\n",
      "Batch:28/88 BatchLoss: G:2.386,D:0.674\n",
      "Batch:29/88 BatchLoss: G:2.115,D:0.718\n",
      "Batch:30/88 BatchLoss: G:1.897,D:0.664\n",
      "Batch:31/88 BatchLoss: G:1.880,D:0.596\n",
      "Batch:32/88 BatchLoss: G:2.254,D:0.614\n",
      "Batch:33/88 BatchLoss: G:2.071,D:0.700\n",
      "Batch:34/88 BatchLoss: G:2.055,D:0.684\n",
      "Batch:35/88 BatchLoss: G:1.875,D:0.789\n",
      "Batch:36/88 BatchLoss: G:1.754,D:0.728\n",
      "Batch:37/88 BatchLoss: G:1.654,D:0.736\n",
      "Batch:38/88 BatchLoss: G:3.537,D:0.709\n",
      "Batch:39/88 BatchLoss: G:1.770,D:0.797\n",
      "Batch:40/88 BatchLoss: G:1.465,D:0.634\n",
      "Batch:41/88 BatchLoss: G:1.420,D:0.694\n",
      "Batch:42/88 BatchLoss: G:1.479,D:0.740\n",
      "Batch:43/88 BatchLoss: G:1.587,D:0.698\n",
      "Batch:44/88 BatchLoss: G:1.488,D:0.793\n",
      "Batch:45/88 BatchLoss: G:1.557,D:0.777\n",
      "Batch:46/88 BatchLoss: G:1.593,D:0.753\n",
      "Batch:47/88 BatchLoss: G:1.701,D:0.708\n",
      "Batch:48/88 BatchLoss: G:2.137,D:0.616\n",
      "Batch:49/88 BatchLoss: G:3.001,D:0.503\n",
      "Batch:50/88 BatchLoss: G:2.508,D:0.530\n",
      "Batch:51/88 BatchLoss: G:2.533,D:0.630\n",
      "Batch:52/88 BatchLoss: G:2.546,D:0.661\n",
      "Batch:53/88 BatchLoss: G:2.755,D:0.594\n",
      "Batch:54/88 BatchLoss: G:2.822,D:0.819\n",
      "Batch:55/88 BatchLoss: G:2.704,D:0.759\n",
      "Batch:56/88 BatchLoss: G:2.549,D:0.753\n",
      "Batch:57/88 BatchLoss: G:2.349,D:0.669\n",
      "Batch:58/88 BatchLoss: G:2.182,D:0.678\n",
      "Batch:59/88 BatchLoss: G:2.196,D:0.591\n",
      "Batch:60/88 BatchLoss: G:1.620,D:0.654\n",
      "Batch:61/88 BatchLoss: G:1.838,D:0.586\n",
      "Batch:62/88 BatchLoss: G:1.796,D:0.682\n",
      "Batch:63/88 BatchLoss: G:1.755,D:0.743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:64/88 BatchLoss: G:1.761,D:0.812\n",
      "Batch:65/88 BatchLoss: G:1.661,D:0.866\n",
      "Batch:66/88 BatchLoss: G:1.630,D:0.829\n",
      "Batch:67/88 BatchLoss: G:1.616,D:0.832\n",
      "Batch:68/88 BatchLoss: G:1.658,D:0.762\n",
      "Batch:69/88 BatchLoss: G:1.753,D:0.767\n",
      "Batch:70/88 BatchLoss: G:2.618,D:0.676\n",
      "Batch:71/88 BatchLoss: G:2.842,D:0.651\n",
      "Batch:72/88 BatchLoss: G:2.462,D:0.755\n",
      "Batch:73/88 BatchLoss: G:2.176,D:0.636\n",
      "Batch:74/88 BatchLoss: G:2.013,D:0.689\n",
      "Batch:75/88 BatchLoss: G:1.964,D:0.776\n",
      "Batch:76/88 BatchLoss: G:2.295,D:0.716\n",
      "Batch:77/88 BatchLoss: G:2.850,D:0.689\n",
      "Batch:78/88 BatchLoss: G:3.235,D:0.770\n",
      "Batch:79/88 BatchLoss: G:2.910,D:0.793\n",
      "Batch:80/88 BatchLoss: G:2.295,D:0.821\n",
      "Batch:81/88 BatchLoss: G:2.199,D:0.615\n",
      "Batch:82/88 BatchLoss: G:2.423,D:0.582\n",
      "Batch:83/88 BatchLoss: G:2.290,D:0.491\n",
      "Batch:84/88 BatchLoss: G:2.154,D:0.499\n",
      "Batch:85/88 BatchLoss: G:2.017,D:0.612\n",
      "Batch:86/88 BatchLoss: G:2.021,D:0.649\n",
      "Batch:87/88 BatchLoss: G:2.099,D:0.639\n",
      "EpochLoss:G:191.28846740722656,D:59.356563568115234\n",
      "Batch:0/88 BatchLoss: G:2.106,D:0.610\n",
      "Batch:1/88 BatchLoss: G:2.026,D:0.611\n",
      "Batch:2/88 BatchLoss: G:2.063,D:0.568\n",
      "Batch:3/88 BatchLoss: G:2.251,D:0.608\n",
      "Batch:4/88 BatchLoss: G:2.966,D:0.566\n",
      "Batch:5/88 BatchLoss: G:2.194,D:0.748\n",
      "Batch:6/88 BatchLoss: G:1.896,D:0.664\n",
      "Batch:7/88 BatchLoss: G:1.968,D:0.586\n",
      "Batch:8/88 BatchLoss: G:2.076,D:0.648\n",
      "Batch:9/88 BatchLoss: G:1.906,D:0.613\n",
      "Batch:10/88 BatchLoss: G:1.684,D:0.688\n",
      "Batch:11/88 BatchLoss: G:1.685,D:0.656\n",
      "Batch:12/88 BatchLoss: G:1.681,D:0.677\n",
      "Batch:13/88 BatchLoss: G:1.991,D:0.654\n",
      "Batch:14/88 BatchLoss: G:2.330,D:0.580\n",
      "Batch:15/88 BatchLoss: G:2.582,D:0.573\n",
      "Batch:16/88 BatchLoss: G:2.091,D:0.624\n",
      "Batch:17/88 BatchLoss: G:2.317,D:0.582\n",
      "Batch:18/88 BatchLoss: G:2.396,D:0.645\n",
      "Batch:19/88 BatchLoss: G:2.642,D:0.563\n",
      "Batch:20/88 BatchLoss: G:2.712,D:0.485\n",
      "Batch:21/88 BatchLoss: G:1.651,D:0.632\n",
      "Batch:22/88 BatchLoss: G:1.714,D:0.546\n",
      "Batch:23/88 BatchLoss: G:1.729,D:0.614\n",
      "Batch:24/88 BatchLoss: G:1.852,D:0.630\n",
      "Batch:25/88 BatchLoss: G:1.954,D:0.655\n",
      "Batch:26/88 BatchLoss: G:2.036,D:0.672\n",
      "Batch:27/88 BatchLoss: G:1.869,D:0.585\n",
      "Batch:28/88 BatchLoss: G:1.619,D:0.742\n",
      "Batch:29/88 BatchLoss: G:1.778,D:0.696\n",
      "Batch:30/88 BatchLoss: G:2.453,D:0.583\n",
      "Batch:31/88 BatchLoss: G:2.046,D:0.661\n",
      "Batch:32/88 BatchLoss: G:1.693,D:0.666\n",
      "Batch:33/88 BatchLoss: G:1.673,D:0.653\n",
      "Batch:34/88 BatchLoss: G:1.962,D:0.734\n",
      "Batch:35/88 BatchLoss: G:1.976,D:0.722\n",
      "Batch:36/88 BatchLoss: G:1.801,D:0.686\n",
      "Batch:37/88 BatchLoss: G:1.836,D:0.638\n",
      "Batch:38/88 BatchLoss: G:2.111,D:0.645\n",
      "Batch:39/88 BatchLoss: G:2.178,D:0.598\n",
      "Batch:40/88 BatchLoss: G:1.934,D:0.588\n",
      "Batch:41/88 BatchLoss: G:1.851,D:0.630\n",
      "Batch:42/88 BatchLoss: G:2.028,D:0.588\n",
      "Batch:43/88 BatchLoss: G:2.086,D:0.614\n",
      "Batch:44/88 BatchLoss: G:1.958,D:0.646\n",
      "Batch:45/88 BatchLoss: G:1.794,D:0.656\n",
      "Batch:46/88 BatchLoss: G:1.811,D:0.693\n",
      "Batch:47/88 BatchLoss: G:2.004,D:0.668\n",
      "Batch:48/88 BatchLoss: G:1.999,D:0.699\n",
      "Batch:49/88 BatchLoss: G:2.068,D:0.667\n",
      "Batch:50/88 BatchLoss: G:2.305,D:0.659\n",
      "Batch:51/88 BatchLoss: G:1.998,D:0.646\n",
      "Batch:52/88 BatchLoss: G:1.653,D:0.738\n",
      "Batch:53/88 BatchLoss: G:2.053,D:0.753\n",
      "Batch:54/88 BatchLoss: G:2.136,D:0.779\n",
      "Batch:55/88 BatchLoss: G:1.747,D:0.760\n",
      "Batch:56/88 BatchLoss: G:1.581,D:0.719\n",
      "Batch:57/88 BatchLoss: G:1.587,D:0.765\n",
      "Batch:58/88 BatchLoss: G:1.562,D:0.826\n",
      "Batch:59/88 BatchLoss: G:1.690,D:0.813\n",
      "Batch:60/88 BatchLoss: G:1.653,D:0.840\n",
      "Batch:61/88 BatchLoss: G:1.631,D:0.757\n",
      "Batch:62/88 BatchLoss: G:2.273,D:0.803\n",
      "Batch:63/88 BatchLoss: G:1.898,D:0.709\n",
      "Batch:64/88 BatchLoss: G:1.470,D:0.733\n",
      "Batch:65/88 BatchLoss: G:1.858,D:0.770\n",
      "Batch:66/88 BatchLoss: G:2.161,D:0.661\n",
      "Batch:67/88 BatchLoss: G:1.638,D:0.784\n",
      "Batch:68/88 BatchLoss: G:1.520,D:0.731\n",
      "Batch:69/88 BatchLoss: G:1.679,D:0.787\n",
      "Batch:70/88 BatchLoss: G:2.446,D:0.769\n",
      "Batch:71/88 BatchLoss: G:1.712,D:0.834\n",
      "Batch:72/88 BatchLoss: G:1.617,D:0.764\n",
      "Batch:73/88 BatchLoss: G:2.203,D:0.816\n",
      "Batch:74/88 BatchLoss: G:1.949,D:0.777\n",
      "Batch:75/88 BatchLoss: G:1.376,D:0.732\n",
      "Batch:76/88 BatchLoss: G:1.831,D:0.863\n",
      "Batch:77/88 BatchLoss: G:2.182,D:0.775\n",
      "Batch:78/88 BatchLoss: G:1.935,D:0.789\n",
      "Batch:79/88 BatchLoss: G:1.480,D:0.787\n",
      "Batch:80/88 BatchLoss: G:1.664,D:0.841\n",
      "Batch:81/88 BatchLoss: G:1.676,D:0.748\n",
      "Batch:82/88 BatchLoss: G:1.801,D:0.942\n",
      "Batch:83/88 BatchLoss: G:2.111,D:0.842\n",
      "Batch:84/88 BatchLoss: G:1.939,D:0.931\n",
      "Batch:85/88 BatchLoss: G:1.787,D:0.847\n",
      "Batch:86/88 BatchLoss: G:1.522,D:1.044\n",
      "Batch:87/88 BatchLoss: G:1.574,D:1.043\n",
      "EpochLoss:G:169.92787170410156,D:61.936222076416016\n",
      "Batch:0/88 BatchLoss: G:1.918,D:0.859\n",
      "Batch:1/88 BatchLoss: G:2.351,D:0.613\n",
      "Batch:2/88 BatchLoss: G:1.900,D:0.505\n",
      "Batch:3/88 BatchLoss: G:1.643,D:0.611\n",
      "Batch:4/88 BatchLoss: G:1.384,D:0.801\n",
      "Batch:5/88 BatchLoss: G:1.300,D:0.939\n",
      "Batch:6/88 BatchLoss: G:1.325,D:0.914\n",
      "Batch:7/88 BatchLoss: G:1.208,D:1.115\n",
      "Batch:8/88 BatchLoss: G:1.363,D:1.026\n",
      "Batch:9/88 BatchLoss: G:1.499,D:1.077\n",
      "Batch:10/88 BatchLoss: G:1.529,D:0.966\n",
      "Batch:11/88 BatchLoss: G:1.790,D:0.857\n",
      "Batch:12/88 BatchLoss: G:1.782,D:0.770\n",
      "Batch:13/88 BatchLoss: G:1.380,D:0.901\n",
      "Batch:14/88 BatchLoss: G:1.760,D:0.873\n",
      "Batch:15/88 BatchLoss: G:2.285,D:0.842\n",
      "Batch:16/88 BatchLoss: G:2.960,D:0.625\n",
      "Batch:17/88 BatchLoss: G:1.538,D:0.838\n",
      "Batch:18/88 BatchLoss: G:1.201,D:0.809\n",
      "Batch:19/88 BatchLoss: G:1.407,D:0.936\n",
      "Batch:20/88 BatchLoss: G:1.484,D:0.845\n",
      "Batch:21/88 BatchLoss: G:1.556,D:0.861\n",
      "Batch:22/88 BatchLoss: G:1.715,D:0.886\n",
      "Batch:23/88 BatchLoss: G:1.661,D:0.931\n",
      "Batch:24/88 BatchLoss: G:1.617,D:0.845\n",
      "Batch:25/88 BatchLoss: G:2.269,D:0.678\n",
      "Batch:26/88 BatchLoss: G:2.018,D:0.660\n",
      "Batch:27/88 BatchLoss: G:1.774,D:0.779\n",
      "Batch:28/88 BatchLoss: G:1.721,D:0.849\n",
      "Batch:29/88 BatchLoss: G:1.898,D:0.814\n",
      "Batch:30/88 BatchLoss: G:1.879,D:0.823\n",
      "Batch:31/88 BatchLoss: G:1.963,D:0.949\n",
      "Batch:32/88 BatchLoss: G:1.818,D:0.975\n",
      "Batch:33/88 BatchLoss: G:1.644,D:1.150\n",
      "Batch:34/88 BatchLoss: G:1.455,D:1.086\n",
      "Batch:35/88 BatchLoss: G:1.433,D:1.141\n",
      "Batch:36/88 BatchLoss: G:1.558,D:1.180\n",
      "Batch:37/88 BatchLoss: G:1.655,D:1.057\n",
      "Batch:38/88 BatchLoss: G:1.740,D:1.148\n",
      "Batch:39/88 BatchLoss: G:1.620,D:1.234\n",
      "Batch:40/88 BatchLoss: G:1.490,D:1.140\n",
      "Batch:41/88 BatchLoss: G:1.460,D:1.067\n",
      "Batch:42/88 BatchLoss: G:1.467,D:1.272\n",
      "Batch:43/88 BatchLoss: G:1.456,D:1.231\n",
      "Batch:44/88 BatchLoss: G:1.529,D:1.120\n",
      "Batch:45/88 BatchLoss: G:1.584,D:1.422\n",
      "Batch:46/88 BatchLoss: G:1.582,D:1.315\n",
      "Batch:47/88 BatchLoss: G:1.552,D:1.207\n",
      "Batch:48/88 BatchLoss: G:1.504,D:1.401\n",
      "Batch:49/88 BatchLoss: G:1.344,D:1.312\n",
      "Batch:50/88 BatchLoss: G:1.497,D:1.189\n",
      "Batch:51/88 BatchLoss: G:2.180,D:1.031\n",
      "Batch:52/88 BatchLoss: G:2.211,D:0.866\n",
      "Batch:53/88 BatchLoss: G:2.084,D:0.976\n",
      "Batch:54/88 BatchLoss: G:1.942,D:0.900\n",
      "Batch:55/88 BatchLoss: G:1.813,D:0.859\n",
      "Batch:56/88 BatchLoss: G:1.658,D:0.913\n",
      "Batch:57/88 BatchLoss: G:1.523,D:0.812\n",
      "Batch:58/88 BatchLoss: G:1.395,D:0.889\n",
      "Batch:59/88 BatchLoss: G:1.257,D:0.926\n",
      "Batch:60/88 BatchLoss: G:1.166,D:1.042\n",
      "Batch:61/88 BatchLoss: G:1.202,D:1.044\n",
      "Batch:62/88 BatchLoss: G:1.198,D:1.103\n",
      "Batch:63/88 BatchLoss: G:1.183,D:1.142\n",
      "Batch:64/88 BatchLoss: G:1.212,D:1.123\n",
      "Batch:65/88 BatchLoss: G:1.205,D:1.176\n",
      "Batch:66/88 BatchLoss: G:1.195,D:1.267\n",
      "Batch:67/88 BatchLoss: G:1.200,D:1.212\n",
      "Batch:68/88 BatchLoss: G:1.179,D:1.294\n",
      "Batch:69/88 BatchLoss: G:1.216,D:1.343\n",
      "Batch:70/88 BatchLoss: G:1.167,D:1.322\n",
      "Batch:71/88 BatchLoss: G:1.209,D:1.405\n",
      "Batch:72/88 BatchLoss: G:1.069,D:1.451\n",
      "Batch:73/88 BatchLoss: G:1.187,D:1.552\n",
      "Batch:74/88 BatchLoss: G:1.114,D:1.534\n",
      "Batch:75/88 BatchLoss: G:1.170,D:1.588\n",
      "Batch:76/88 BatchLoss: G:1.115,D:1.552\n",
      "Batch:77/88 BatchLoss: G:1.178,D:1.541\n",
      "Batch:78/88 BatchLoss: G:1.193,D:1.513\n",
      "Batch:79/88 BatchLoss: G:1.237,D:1.496\n",
      "Batch:80/88 BatchLoss: G:1.271,D:1.427\n",
      "Batch:81/88 BatchLoss: G:1.347,D:1.408\n",
      "Batch:82/88 BatchLoss: G:1.333,D:1.297\n",
      "Batch:83/88 BatchLoss: G:1.435,D:1.156\n",
      "Batch:84/88 BatchLoss: G:1.432,D:1.019\n",
      "Batch:85/88 BatchLoss: G:1.414,D:0.869\n",
      "Batch:86/88 BatchLoss: G:1.892,D:0.805\n",
      "Batch:87/88 BatchLoss: G:2.781,D:0.638\n",
      "EpochLoss:G:137.0302276611328,D:92.93717193603516\n",
      "Batch:0/88 BatchLoss: G:2.662,D:0.755\n",
      "Batch:1/88 BatchLoss: G:2.505,D:0.793\n",
      "Batch:2/88 BatchLoss: G:2.327,D:0.814\n",
      "Batch:3/88 BatchLoss: G:2.196,D:0.825\n",
      "Batch:4/88 BatchLoss: G:1.932,D:0.865\n",
      "Batch:5/88 BatchLoss: G:1.854,D:0.926\n",
      "Batch:6/88 BatchLoss: G:1.683,D:0.928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:7/88 BatchLoss: G:1.578,D:0.955\n",
      "Batch:8/88 BatchLoss: G:1.421,D:1.014\n",
      "Batch:9/88 BatchLoss: G:1.418,D:1.094\n",
      "Batch:10/88 BatchLoss: G:1.341,D:1.009\n",
      "Batch:11/88 BatchLoss: G:1.239,D:1.113\n",
      "Batch:12/88 BatchLoss: G:1.290,D:1.064\n",
      "Batch:13/88 BatchLoss: G:1.263,D:1.165\n",
      "Batch:14/88 BatchLoss: G:1.254,D:1.193\n",
      "Batch:15/88 BatchLoss: G:1.239,D:1.251\n",
      "Batch:16/88 BatchLoss: G:1.255,D:1.231\n",
      "Batch:17/88 BatchLoss: G:1.259,D:1.201\n",
      "Batch:18/88 BatchLoss: G:1.182,D:1.232\n",
      "Batch:19/88 BatchLoss: G:1.231,D:1.230\n",
      "Batch:20/88 BatchLoss: G:1.214,D:1.262\n",
      "Batch:21/88 BatchLoss: G:1.208,D:1.253\n",
      "Batch:22/88 BatchLoss: G:1.174,D:1.308\n",
      "Batch:23/88 BatchLoss: G:1.202,D:1.250\n",
      "Batch:24/88 BatchLoss: G:1.252,D:1.288\n",
      "Batch:25/88 BatchLoss: G:1.230,D:1.284\n",
      "Batch:26/88 BatchLoss: G:1.283,D:1.203\n",
      "Batch:27/88 BatchLoss: G:1.165,D:1.291\n",
      "Batch:28/88 BatchLoss: G:1.364,D:1.342\n",
      "Batch:29/88 BatchLoss: G:1.272,D:1.319\n",
      "Batch:30/88 BatchLoss: G:1.363,D:1.332\n",
      "Batch:31/88 BatchLoss: G:1.343,D:1.326\n",
      "Batch:32/88 BatchLoss: G:1.369,D:1.369\n",
      "Batch:33/88 BatchLoss: G:1.430,D:1.357\n",
      "Batch:34/88 BatchLoss: G:1.488,D:1.335\n",
      "Batch:35/88 BatchLoss: G:1.353,D:1.293\n",
      "Batch:36/88 BatchLoss: G:1.449,D:1.120\n",
      "Batch:37/88 BatchLoss: G:1.502,D:1.141\n",
      "Batch:38/88 BatchLoss: G:1.635,D:1.106\n",
      "Batch:39/88 BatchLoss: G:1.705,D:1.135\n",
      "Batch:40/88 BatchLoss: G:1.789,D:1.137\n",
      "Batch:41/88 BatchLoss: G:1.668,D:1.150\n",
      "Batch:42/88 BatchLoss: G:1.616,D:1.143\n",
      "Batch:43/88 BatchLoss: G:1.518,D:1.059\n",
      "Batch:44/88 BatchLoss: G:1.544,D:1.099\n",
      "Batch:45/88 BatchLoss: G:1.435,D:1.138\n",
      "Batch:46/88 BatchLoss: G:1.294,D:1.163\n",
      "Batch:47/88 BatchLoss: G:1.224,D:1.166\n",
      "Batch:48/88 BatchLoss: G:1.255,D:1.151\n",
      "Batch:49/88 BatchLoss: G:1.130,D:1.252\n",
      "Batch:50/88 BatchLoss: G:1.061,D:1.292\n",
      "Batch:51/88 BatchLoss: G:1.048,D:1.353\n",
      "Batch:52/88 BatchLoss: G:1.089,D:1.387\n",
      "Batch:53/88 BatchLoss: G:1.092,D:1.439\n",
      "Batch:54/88 BatchLoss: G:1.081,D:1.457\n",
      "Batch:55/88 BatchLoss: G:1.144,D:1.441\n",
      "Batch:56/88 BatchLoss: G:1.251,D:1.406\n",
      "Batch:57/88 BatchLoss: G:1.396,D:1.456\n",
      "Batch:58/88 BatchLoss: G:1.491,D:1.478\n",
      "Batch:59/88 BatchLoss: G:1.552,D:1.473\n",
      "Batch:60/88 BatchLoss: G:1.564,D:1.731\n",
      "Batch:61/88 BatchLoss: G:1.466,D:1.517\n",
      "Batch:62/88 BatchLoss: G:1.504,D:1.736\n",
      "Batch:63/88 BatchLoss: G:1.511,D:1.633\n",
      "Batch:64/88 BatchLoss: G:1.428,D:1.735\n",
      "Batch:65/88 BatchLoss: G:1.414,D:1.681\n",
      "Batch:66/88 BatchLoss: G:1.246,D:1.528\n",
      "Batch:67/88 BatchLoss: G:1.204,D:1.336\n",
      "Batch:68/88 BatchLoss: G:1.060,D:1.319\n",
      "Batch:69/88 BatchLoss: G:1.085,D:1.273\n",
      "Batch:70/88 BatchLoss: G:1.401,D:1.288\n",
      "Batch:71/88 BatchLoss: G:4.020,D:0.899\n",
      "Batch:72/88 BatchLoss: G:3.727,D:0.776\n",
      "Batch:73/88 BatchLoss: G:2.986,D:1.067\n",
      "Batch:74/88 BatchLoss: G:2.297,D:1.064\n",
      "Batch:75/88 BatchLoss: G:2.069,D:0.853\n",
      "Batch:76/88 BatchLoss: G:1.884,D:0.808\n",
      "Batch:77/88 BatchLoss: G:1.818,D:0.814\n",
      "Batch:78/88 BatchLoss: G:1.636,D:0.939\n",
      "Batch:79/88 BatchLoss: G:1.606,D:0.911\n",
      "Batch:80/88 BatchLoss: G:1.563,D:0.880\n",
      "Batch:81/88 BatchLoss: G:1.505,D:0.978\n",
      "Batch:82/88 BatchLoss: G:1.395,D:1.061\n",
      "Batch:83/88 BatchLoss: G:1.321,D:1.048\n",
      "Batch:84/88 BatchLoss: G:1.328,D:1.112\n",
      "Batch:85/88 BatchLoss: G:1.307,D:1.055\n",
      "Batch:86/88 BatchLoss: G:1.369,D:1.040\n",
      "Batch:87/88 BatchLoss: G:1.346,D:1.203\n",
      "EpochLoss:G:133.86776733398438,D:104.89923858642578\n",
      "Batch:0/88 BatchLoss: G:1.329,D:1.080\n",
      "Batch:1/88 BatchLoss: G:1.311,D:1.182\n",
      "Batch:2/88 BatchLoss: G:1.273,D:1.125\n",
      "Batch:3/88 BatchLoss: G:1.249,D:1.154\n",
      "Batch:4/88 BatchLoss: G:1.305,D:1.186\n",
      "Batch:5/88 BatchLoss: G:1.294,D:1.184\n",
      "Batch:6/88 BatchLoss: G:1.315,D:1.230\n",
      "Batch:7/88 BatchLoss: G:1.304,D:1.183\n",
      "Batch:8/88 BatchLoss: G:1.277,D:1.269\n",
      "Batch:9/88 BatchLoss: G:1.289,D:1.219\n",
      "Batch:10/88 BatchLoss: G:1.271,D:1.231\n",
      "Batch:11/88 BatchLoss: G:1.288,D:1.222\n",
      "Batch:12/88 BatchLoss: G:1.324,D:1.270\n",
      "Batch:13/88 BatchLoss: G:1.383,D:1.302\n",
      "Batch:14/88 BatchLoss: G:1.354,D:1.294\n",
      "Batch:15/88 BatchLoss: G:1.363,D:1.288\n",
      "Batch:16/88 BatchLoss: G:1.379,D:1.326\n",
      "Batch:17/88 BatchLoss: G:1.308,D:1.294\n",
      "Batch:18/88 BatchLoss: G:1.349,D:1.308\n",
      "Batch:19/88 BatchLoss: G:1.440,D:1.243\n",
      "Batch:20/88 BatchLoss: G:1.443,D:1.253\n",
      "Batch:21/88 BatchLoss: G:1.719,D:1.206\n",
      "Batch:22/88 BatchLoss: G:2.039,D:1.165\n",
      "Batch:23/88 BatchLoss: G:2.264,D:1.155\n",
      "Batch:24/88 BatchLoss: G:2.491,D:1.057\n",
      "Batch:25/88 BatchLoss: G:2.173,D:1.288\n",
      "Batch:26/88 BatchLoss: G:1.903,D:1.292\n",
      "Batch:27/88 BatchLoss: G:1.506,D:1.173\n",
      "Batch:28/88 BatchLoss: G:1.283,D:1.235\n",
      "Batch:29/88 BatchLoss: G:1.047,D:1.459\n",
      "Batch:30/88 BatchLoss: G:1.171,D:1.798\n",
      "Batch:31/88 BatchLoss: G:1.831,D:1.664\n",
      "Batch:32/88 BatchLoss: G:2.662,D:1.183\n",
      "Batch:33/88 BatchLoss: G:2.795,D:1.098\n",
      "Batch:34/88 BatchLoss: G:2.643,D:1.270\n",
      "Batch:35/88 BatchLoss: G:2.558,D:1.340\n",
      "Batch:36/88 BatchLoss: G:2.504,D:1.289\n",
      "Batch:37/88 BatchLoss: G:2.421,D:1.314\n",
      "Batch:38/88 BatchLoss: G:2.327,D:1.302\n",
      "Batch:39/88 BatchLoss: G:2.139,D:1.227\n",
      "Batch:40/88 BatchLoss: G:2.084,D:1.480\n",
      "Batch:41/88 BatchLoss: G:2.050,D:1.298\n",
      "Batch:42/88 BatchLoss: G:1.887,D:1.292\n",
      "Batch:43/88 BatchLoss: G:1.855,D:1.363\n",
      "Batch:44/88 BatchLoss: G:1.747,D:1.354\n",
      "Batch:45/88 BatchLoss: G:1.718,D:1.291\n",
      "Batch:46/88 BatchLoss: G:1.625,D:1.370\n",
      "Batch:47/88 BatchLoss: G:1.554,D:1.288\n",
      "Batch:48/88 BatchLoss: G:1.488,D:1.279\n",
      "Batch:49/88 BatchLoss: G:1.400,D:1.417\n",
      "Batch:50/88 BatchLoss: G:1.417,D:1.468\n",
      "Batch:51/88 BatchLoss: G:1.307,D:1.464\n",
      "Batch:52/88 BatchLoss: G:1.322,D:1.502\n",
      "Batch:53/88 BatchLoss: G:1.250,D:1.534\n",
      "Batch:54/88 BatchLoss: G:1.193,D:1.653\n",
      "Batch:55/88 BatchLoss: G:1.146,D:1.761\n",
      "Batch:56/88 BatchLoss: G:1.135,D:1.845\n",
      "Batch:57/88 BatchLoss: G:1.090,D:1.802\n",
      "Batch:58/88 BatchLoss: G:1.046,D:1.845\n",
      "Batch:59/88 BatchLoss: G:1.015,D:1.893\n",
      "Batch:60/88 BatchLoss: G:1.005,D:1.988\n",
      "Batch:61/88 BatchLoss: G:0.953,D:1.986\n",
      "Batch:62/88 BatchLoss: G:0.940,D:2.109\n",
      "Batch:63/88 BatchLoss: G:0.913,D:2.049\n",
      "Batch:64/88 BatchLoss: G:0.907,D:2.036\n",
      "Batch:65/88 BatchLoss: G:0.975,D:2.073\n",
      "Batch:66/88 BatchLoss: G:0.993,D:1.989\n",
      "Batch:67/88 BatchLoss: G:1.039,D:1.864\n",
      "Batch:68/88 BatchLoss: G:1.106,D:1.801\n",
      "Batch:69/88 BatchLoss: G:1.101,D:1.768\n",
      "Batch:70/88 BatchLoss: G:1.224,D:1.717\n",
      "Batch:71/88 BatchLoss: G:1.315,D:1.691\n",
      "Batch:72/88 BatchLoss: G:1.415,D:1.694\n",
      "Batch:73/88 BatchLoss: G:1.404,D:1.616\n",
      "Batch:74/88 BatchLoss: G:1.414,D:1.593\n",
      "Batch:75/88 BatchLoss: G:1.560,D:1.698\n",
      "Batch:76/88 BatchLoss: G:1.598,D:1.691\n",
      "Batch:77/88 BatchLoss: G:1.601,D:1.654\n",
      "Batch:78/88 BatchLoss: G:1.575,D:1.613\n",
      "Batch:79/88 BatchLoss: G:1.802,D:1.760\n",
      "Batch:80/88 BatchLoss: G:1.854,D:1.735\n",
      "Batch:81/88 BatchLoss: G:1.857,D:1.664\n",
      "Batch:82/88 BatchLoss: G:1.972,D:1.867\n",
      "Batch:83/88 BatchLoss: G:1.729,D:1.630\n",
      "Batch:84/88 BatchLoss: G:1.730,D:1.649\n",
      "Batch:85/88 BatchLoss: G:1.739,D:1.668\n",
      "Batch:86/88 BatchLoss: G:1.664,D:1.660\n",
      "Batch:87/88 BatchLoss: G:1.625,D:1.632\n",
      "EpochLoss:G:135.6622314453125,D:130.45089721679688\n",
      "Batch:0/88 BatchLoss: G:1.563,D:1.552\n",
      "Batch:1/88 BatchLoss: G:1.411,D:1.477\n",
      "Batch:2/88 BatchLoss: G:1.489,D:1.505\n",
      "Batch:3/88 BatchLoss: G:1.470,D:1.453\n",
      "Batch:4/88 BatchLoss: G:1.371,D:1.403\n",
      "Batch:5/88 BatchLoss: G:1.392,D:1.340\n",
      "Batch:6/88 BatchLoss: G:1.479,D:1.303\n",
      "Batch:7/88 BatchLoss: G:1.379,D:1.265\n",
      "Batch:8/88 BatchLoss: G:1.543,D:1.198\n",
      "Batch:9/88 BatchLoss: G:2.135,D:1.128\n",
      "Batch:10/88 BatchLoss: G:2.217,D:1.109\n",
      "Batch:11/88 BatchLoss: G:2.353,D:1.035\n",
      "Batch:12/88 BatchLoss: G:2.143,D:1.072\n",
      "Batch:13/88 BatchLoss: G:1.998,D:1.072\n",
      "Batch:14/88 BatchLoss: G:1.852,D:0.969\n",
      "Batch:15/88 BatchLoss: G:1.850,D:0.918\n",
      "Batch:16/88 BatchLoss: G:1.676,D:0.953\n",
      "Batch:17/88 BatchLoss: G:1.515,D:1.018\n",
      "Batch:18/88 BatchLoss: G:1.419,D:0.948\n",
      "Batch:19/88 BatchLoss: G:1.374,D:0.969\n",
      "Batch:20/88 BatchLoss: G:1.278,D:1.044\n",
      "Batch:21/88 BatchLoss: G:1.212,D:1.069\n",
      "Batch:22/88 BatchLoss: G:1.230,D:1.038\n",
      "Batch:23/88 BatchLoss: G:1.164,D:1.221\n",
      "Batch:24/88 BatchLoss: G:1.096,D:1.250\n",
      "Batch:25/88 BatchLoss: G:1.073,D:1.273\n",
      "Batch:26/88 BatchLoss: G:1.054,D:1.292\n",
      "Batch:27/88 BatchLoss: G:1.001,D:1.289\n",
      "Batch:28/88 BatchLoss: G:0.994,D:1.394\n",
      "Batch:29/88 BatchLoss: G:0.971,D:1.396\n",
      "Batch:30/88 BatchLoss: G:0.988,D:1.391\n",
      "Batch:31/88 BatchLoss: G:0.953,D:1.490\n",
      "Batch:32/88 BatchLoss: G:0.943,D:1.641\n",
      "Batch:33/88 BatchLoss: G:0.911,D:1.623\n",
      "Batch:34/88 BatchLoss: G:0.975,D:1.829\n",
      "Batch:35/88 BatchLoss: G:0.930,D:1.811\n",
      "Batch:36/88 BatchLoss: G:0.931,D:1.875\n",
      "Batch:37/88 BatchLoss: G:0.873,D:1.798\n",
      "Batch:38/88 BatchLoss: G:0.896,D:2.019\n",
      "Batch:39/88 BatchLoss: G:0.874,D:2.170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:40/88 BatchLoss: G:0.794,D:2.027\n",
      "Batch:41/88 BatchLoss: G:0.806,D:2.080\n",
      "Batch:42/88 BatchLoss: G:0.793,D:2.133\n",
      "Batch:43/88 BatchLoss: G:0.801,D:2.233\n",
      "Batch:44/88 BatchLoss: G:0.799,D:2.155\n",
      "Batch:45/88 BatchLoss: G:0.741,D:2.138\n",
      "Batch:46/88 BatchLoss: G:0.759,D:2.164\n",
      "Batch:47/88 BatchLoss: G:0.814,D:2.266\n",
      "Batch:48/88 BatchLoss: G:0.794,D:2.255\n",
      "Batch:49/88 BatchLoss: G:0.785,D:2.327\n",
      "Batch:50/88 BatchLoss: G:0.822,D:2.329\n",
      "Batch:51/88 BatchLoss: G:0.895,D:2.436\n",
      "Batch:52/88 BatchLoss: G:0.942,D:2.401\n",
      "Batch:53/88 BatchLoss: G:0.909,D:2.443\n",
      "Batch:54/88 BatchLoss: G:0.992,D:2.315\n",
      "Batch:55/88 BatchLoss: G:1.326,D:2.333\n",
      "Batch:56/88 BatchLoss: G:2.564,D:2.195\n",
      "Batch:57/88 BatchLoss: G:1.543,D:3.192\n",
      "Batch:58/88 BatchLoss: G:1.058,D:2.362\n",
      "Batch:59/88 BatchLoss: G:0.976,D:2.574\n",
      "Batch:60/88 BatchLoss: G:0.949,D:2.983\n",
      "Batch:61/88 BatchLoss: G:0.888,D:2.663\n",
      "Batch:62/88 BatchLoss: G:0.910,D:2.701\n",
      "Batch:63/88 BatchLoss: G:0.842,D:2.679\n",
      "Batch:64/88 BatchLoss: G:0.863,D:2.688\n",
      "Batch:65/88 BatchLoss: G:0.850,D:2.620\n",
      "Batch:66/88 BatchLoss: G:0.784,D:2.641\n",
      "Batch:67/88 BatchLoss: G:0.832,D:2.926\n",
      "Batch:68/88 BatchLoss: G:0.837,D:3.003\n",
      "Batch:69/88 BatchLoss: G:0.746,D:2.753\n",
      "Batch:70/88 BatchLoss: G:0.786,D:3.063\n",
      "Batch:71/88 BatchLoss: G:0.712,D:3.022\n",
      "Batch:72/88 BatchLoss: G:0.694,D:3.128\n",
      "Batch:73/88 BatchLoss: G:0.693,D:3.124\n",
      "Batch:74/88 BatchLoss: G:0.777,D:3.425\n",
      "Batch:75/88 BatchLoss: G:0.736,D:3.295\n",
      "Batch:76/88 BatchLoss: G:0.668,D:3.237\n",
      "Batch:77/88 BatchLoss: G:0.657,D:3.179\n",
      "Batch:78/88 BatchLoss: G:0.756,D:3.002\n",
      "Batch:79/88 BatchLoss: G:0.918,D:2.849\n",
      "Batch:80/88 BatchLoss: G:1.070,D:2.509\n",
      "Batch:81/88 BatchLoss: G:1.282,D:2.264\n",
      "Batch:82/88 BatchLoss: G:1.557,D:2.516\n",
      "Batch:83/88 BatchLoss: G:1.683,D:2.523\n",
      "Batch:84/88 BatchLoss: G:1.564,D:2.471\n",
      "Batch:85/88 BatchLoss: G:1.542,D:2.408\n",
      "Batch:86/88 BatchLoss: G:1.518,D:2.476\n",
      "Batch:87/88 BatchLoss: G:1.538,D:2.358\n",
      "EpochLoss:G:101.54280853271484,D:178.46307373046875\n",
      "Batch:0/88 BatchLoss: G:1.354,D:2.490\n",
      "Batch:1/88 BatchLoss: G:1.359,D:2.247\n",
      "Batch:2/88 BatchLoss: G:1.286,D:2.374\n",
      "Batch:3/88 BatchLoss: G:1.233,D:2.172\n",
      "Batch:4/88 BatchLoss: G:1.163,D:2.058\n",
      "Batch:5/88 BatchLoss: G:1.044,D:2.092\n",
      "Batch:6/88 BatchLoss: G:1.162,D:2.352\n",
      "Batch:7/88 BatchLoss: G:1.063,D:2.034\n",
      "Batch:8/88 BatchLoss: G:0.950,D:2.059\n",
      "Batch:9/88 BatchLoss: G:1.008,D:2.047\n",
      "Batch:10/88 BatchLoss: G:1.009,D:2.019\n",
      "Batch:11/88 BatchLoss: G:1.120,D:2.277\n",
      "Batch:12/88 BatchLoss: G:1.041,D:1.992\n",
      "Batch:13/88 BatchLoss: G:1.036,D:1.951\n",
      "Batch:14/88 BatchLoss: G:1.064,D:1.911\n",
      "Batch:15/88 BatchLoss: G:1.029,D:1.910\n",
      "Batch:16/88 BatchLoss: G:1.016,D:1.683\n",
      "Batch:17/88 BatchLoss: G:1.084,D:1.590\n",
      "Batch:18/88 BatchLoss: G:1.116,D:1.388\n",
      "Batch:19/88 BatchLoss: G:1.157,D:1.290\n",
      "Batch:20/88 BatchLoss: G:1.117,D:1.303\n",
      "Batch:21/88 BatchLoss: G:1.139,D:1.281\n",
      "Batch:22/88 BatchLoss: G:1.144,D:1.280\n",
      "Batch:23/88 BatchLoss: G:1.224,D:1.239\n",
      "Batch:24/88 BatchLoss: G:1.231,D:1.227\n",
      "Batch:25/88 BatchLoss: G:1.218,D:1.261\n",
      "Batch:26/88 BatchLoss: G:1.199,D:1.245\n",
      "Batch:27/88 BatchLoss: G:1.215,D:1.280\n",
      "Batch:28/88 BatchLoss: G:1.260,D:1.249\n",
      "Batch:29/88 BatchLoss: G:1.161,D:1.323\n",
      "Batch:30/88 BatchLoss: G:1.208,D:1.324\n",
      "Batch:31/88 BatchLoss: G:1.175,D:1.351\n",
      "Batch:32/88 BatchLoss: G:1.150,D:1.399\n",
      "Batch:33/88 BatchLoss: G:1.208,D:1.348\n",
      "Batch:34/88 BatchLoss: G:1.104,D:1.433\n",
      "Batch:35/88 BatchLoss: G:1.115,D:1.421\n",
      "Batch:36/88 BatchLoss: G:1.057,D:1.531\n",
      "Batch:37/88 BatchLoss: G:1.030,D:1.545\n",
      "Batch:38/88 BatchLoss: G:1.036,D:1.546\n",
      "Batch:39/88 BatchLoss: G:0.993,D:1.590\n",
      "Batch:40/88 BatchLoss: G:1.031,D:1.603\n",
      "Batch:41/88 BatchLoss: G:0.967,D:1.683\n",
      "Batch:42/88 BatchLoss: G:1.013,D:1.643\n",
      "Batch:43/88 BatchLoss: G:0.988,D:1.678\n",
      "Batch:44/88 BatchLoss: G:1.012,D:1.712\n",
      "Batch:45/88 BatchLoss: G:1.007,D:1.716\n",
      "Batch:46/88 BatchLoss: G:1.123,D:1.732\n",
      "Batch:47/88 BatchLoss: G:1.176,D:1.766\n",
      "Batch:48/88 BatchLoss: G:1.407,D:1.653\n",
      "Batch:49/88 BatchLoss: G:2.117,D:1.470\n",
      "Batch:50/88 BatchLoss: G:1.640,D:1.809\n",
      "Batch:51/88 BatchLoss: G:1.267,D:1.698\n",
      "Batch:52/88 BatchLoss: G:1.211,D:1.667\n",
      "Batch:53/88 BatchLoss: G:1.115,D:1.629\n",
      "Batch:54/88 BatchLoss: G:1.024,D:1.619\n",
      "Batch:55/88 BatchLoss: G:1.067,D:1.682\n",
      "Batch:56/88 BatchLoss: G:1.055,D:1.662\n",
      "Batch:57/88 BatchLoss: G:1.048,D:1.669\n",
      "Batch:58/88 BatchLoss: G:0.977,D:1.692\n",
      "Batch:59/88 BatchLoss: G:1.031,D:1.629\n",
      "Batch:60/88 BatchLoss: G:1.079,D:1.587\n",
      "Batch:61/88 BatchLoss: G:1.121,D:1.591\n",
      "Batch:62/88 BatchLoss: G:1.160,D:1.486\n",
      "Batch:63/88 BatchLoss: G:1.219,D:1.419\n",
      "Batch:64/88 BatchLoss: G:1.182,D:1.368\n",
      "Batch:65/88 BatchLoss: G:1.293,D:1.330\n",
      "Batch:66/88 BatchLoss: G:1.400,D:1.309\n",
      "Batch:67/88 BatchLoss: G:1.549,D:1.258\n",
      "Batch:68/88 BatchLoss: G:1.698,D:1.227\n",
      "Batch:69/88 BatchLoss: G:1.810,D:1.268\n",
      "Batch:70/88 BatchLoss: G:1.749,D:1.387\n",
      "Batch:71/88 BatchLoss: G:1.596,D:1.317\n",
      "Batch:72/88 BatchLoss: G:1.472,D:1.392\n",
      "Batch:73/88 BatchLoss: G:1.329,D:1.497\n",
      "Batch:74/88 BatchLoss: G:1.193,D:1.419\n",
      "Batch:75/88 BatchLoss: G:1.097,D:1.552\n",
      "Batch:76/88 BatchLoss: G:1.040,D:1.592\n",
      "Batch:77/88 BatchLoss: G:0.996,D:1.628\n",
      "Batch:78/88 BatchLoss: G:0.947,D:1.793\n",
      "Batch:79/88 BatchLoss: G:0.941,D:1.867\n",
      "Batch:80/88 BatchLoss: G:0.947,D:1.994\n",
      "Batch:81/88 BatchLoss: G:0.911,D:2.034\n",
      "Batch:82/88 BatchLoss: G:0.928,D:2.061\n",
      "Batch:83/88 BatchLoss: G:0.986,D:2.097\n",
      "Batch:84/88 BatchLoss: G:1.078,D:2.030\n",
      "Batch:85/88 BatchLoss: G:1.178,D:1.967\n",
      "Batch:86/88 BatchLoss: G:1.223,D:1.912\n",
      "Batch:87/88 BatchLoss: G:1.238,D:1.968\n",
      "EpochLoss:G:102.9150161743164,D:145.8780517578125\n",
      "Batch:0/88 BatchLoss: G:1.380,D:2.061\n",
      "Batch:1/88 BatchLoss: G:1.249,D:1.965\n",
      "Batch:2/88 BatchLoss: G:1.372,D:1.950\n",
      "Batch:3/88 BatchLoss: G:1.325,D:1.913\n",
      "Batch:4/88 BatchLoss: G:1.327,D:1.898\n",
      "Batch:5/88 BatchLoss: G:1.338,D:2.032\n",
      "Batch:6/88 BatchLoss: G:1.268,D:1.751\n",
      "Batch:7/88 BatchLoss: G:1.348,D:1.654\n",
      "Batch:8/88 BatchLoss: G:1.228,D:1.411\n",
      "Batch:9/88 BatchLoss: G:1.198,D:1.340\n",
      "Batch:10/88 BatchLoss: G:1.256,D:1.292\n",
      "Batch:11/88 BatchLoss: G:1.217,D:1.289\n",
      "Batch:12/88 BatchLoss: G:1.190,D:1.275\n",
      "Batch:13/88 BatchLoss: G:1.227,D:1.255\n",
      "Batch:14/88 BatchLoss: G:1.208,D:1.274\n",
      "Batch:15/88 BatchLoss: G:1.211,D:1.256\n",
      "Batch:16/88 BatchLoss: G:1.253,D:1.225\n",
      "Batch:17/88 BatchLoss: G:1.204,D:1.247\n",
      "Batch:18/88 BatchLoss: G:1.231,D:1.216\n",
      "Batch:19/88 BatchLoss: G:1.146,D:1.266\n",
      "Batch:20/88 BatchLoss: G:1.138,D:1.288\n",
      "Batch:21/88 BatchLoss: G:1.144,D:1.263\n",
      "Batch:22/88 BatchLoss: G:1.158,D:1.260\n",
      "Batch:23/88 BatchLoss: G:1.084,D:1.323\n",
      "Batch:24/88 BatchLoss: G:1.061,D:1.340\n",
      "Batch:25/88 BatchLoss: G:1.030,D:1.365\n",
      "Batch:26/88 BatchLoss: G:1.022,D:1.427\n",
      "Batch:27/88 BatchLoss: G:1.007,D:1.471\n",
      "Batch:28/88 BatchLoss: G:1.005,D:1.459\n",
      "Batch:29/88 BatchLoss: G:1.018,D:1.479\n",
      "Batch:30/88 BatchLoss: G:0.939,D:1.541\n",
      "Batch:31/88 BatchLoss: G:0.989,D:1.549\n",
      "Batch:32/88 BatchLoss: G:0.999,D:1.574\n",
      "Batch:33/88 BatchLoss: G:0.973,D:1.585\n",
      "Batch:34/88 BatchLoss: G:0.955,D:1.614\n",
      "Batch:35/88 BatchLoss: G:0.971,D:1.601\n",
      "Batch:36/88 BatchLoss: G:0.999,D:1.673\n",
      "Batch:37/88 BatchLoss: G:0.960,D:1.667\n",
      "Batch:38/88 BatchLoss: G:1.006,D:1.656\n",
      "Batch:39/88 BatchLoss: G:1.014,D:1.686\n",
      "Batch:40/88 BatchLoss: G:1.030,D:1.668\n",
      "Batch:41/88 BatchLoss: G:1.081,D:1.653\n",
      "Batch:42/88 BatchLoss: G:1.015,D:1.681\n",
      "Batch:43/88 BatchLoss: G:1.057,D:1.632\n",
      "Batch:44/88 BatchLoss: G:1.065,D:1.611\n",
      "Batch:45/88 BatchLoss: G:1.081,D:1.551\n",
      "Batch:46/88 BatchLoss: G:1.093,D:1.587\n",
      "Batch:47/88 BatchLoss: G:1.109,D:1.562\n",
      "Batch:48/88 BatchLoss: G:1.218,D:1.593\n",
      "Batch:49/88 BatchLoss: G:1.109,D:1.512\n",
      "Batch:50/88 BatchLoss: G:1.193,D:1.528\n",
      "Batch:51/88 BatchLoss: G:1.234,D:1.456\n",
      "Batch:52/88 BatchLoss: G:1.262,D:1.457\n",
      "Batch:53/88 BatchLoss: G:1.145,D:1.431\n",
      "Batch:54/88 BatchLoss: G:1.233,D:1.411\n",
      "Batch:55/88 BatchLoss: G:1.240,D:1.382\n",
      "Batch:56/88 BatchLoss: G:1.142,D:1.398\n",
      "Batch:57/88 BatchLoss: G:1.250,D:1.363\n",
      "Batch:58/88 BatchLoss: G:1.224,D:1.379\n",
      "Batch:59/88 BatchLoss: G:1.123,D:1.368\n",
      "Batch:60/88 BatchLoss: G:1.107,D:1.378\n",
      "Batch:61/88 BatchLoss: G:1.118,D:1.389\n",
      "Batch:62/88 BatchLoss: G:1.068,D:1.391\n",
      "Batch:63/88 BatchLoss: G:1.004,D:1.411\n",
      "Batch:64/88 BatchLoss: G:1.072,D:1.420\n",
      "Batch:65/88 BatchLoss: G:1.072,D:1.443\n",
      "Batch:66/88 BatchLoss: G:1.152,D:1.457\n",
      "Batch:67/88 BatchLoss: G:1.187,D:1.490\n",
      "Batch:68/88 BatchLoss: G:1.212,D:1.613\n",
      "Batch:69/88 BatchLoss: G:1.125,D:1.534\n",
      "Batch:70/88 BatchLoss: G:1.142,D:1.454\n",
      "Batch:71/88 BatchLoss: G:1.148,D:1.561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:72/88 BatchLoss: G:1.072,D:1.654\n",
      "Batch:73/88 BatchLoss: G:1.060,D:1.708\n",
      "Batch:74/88 BatchLoss: G:0.997,D:1.595\n",
      "Batch:75/88 BatchLoss: G:1.001,D:1.498\n",
      "Batch:76/88 BatchLoss: G:0.928,D:1.530\n",
      "Batch:77/88 BatchLoss: G:0.980,D:1.496\n",
      "Batch:78/88 BatchLoss: G:1.042,D:1.492\n",
      "Batch:79/88 BatchLoss: G:0.991,D:1.459\n",
      "Batch:80/88 BatchLoss: G:1.018,D:1.442\n",
      "Batch:81/88 BatchLoss: G:1.036,D:1.451\n",
      "Batch:82/88 BatchLoss: G:1.005,D:1.434\n",
      "Batch:83/88 BatchLoss: G:1.142,D:1.425\n",
      "Batch:84/88 BatchLoss: G:1.126,D:1.422\n",
      "Batch:85/88 BatchLoss: G:1.051,D:1.384\n",
      "Batch:86/88 BatchLoss: G:1.055,D:1.404\n",
      "Batch:87/88 BatchLoss: G:1.163,D:1.401\n",
      "EpochLoss:G:98.6280288696289,D:131.95030212402344\n",
      "Batch:0/88 BatchLoss: G:1.071,D:1.397\n",
      "Batch:1/88 BatchLoss: G:1.132,D:1.370\n",
      "Batch:2/88 BatchLoss: G:1.128,D:1.385\n",
      "Batch:3/88 BatchLoss: G:1.202,D:1.398\n",
      "Batch:4/88 BatchLoss: G:1.245,D:1.414\n",
      "Batch:5/88 BatchLoss: G:1.144,D:1.383\n",
      "Batch:6/88 BatchLoss: G:1.169,D:1.406\n",
      "Batch:7/88 BatchLoss: G:1.066,D:1.396\n",
      "Batch:8/88 BatchLoss: G:1.152,D:1.410\n",
      "Batch:9/88 BatchLoss: G:1.099,D:1.392\n",
      "Batch:10/88 BatchLoss: G:1.104,D:1.432\n",
      "Batch:11/88 BatchLoss: G:1.043,D:1.402\n",
      "Batch:12/88 BatchLoss: G:1.028,D:1.428\n",
      "Batch:13/88 BatchLoss: G:1.043,D:1.424\n",
      "Batch:14/88 BatchLoss: G:0.977,D:1.428\n",
      "Batch:15/88 BatchLoss: G:1.094,D:1.454\n",
      "Batch:16/88 BatchLoss: G:0.969,D:1.455\n",
      "Batch:17/88 BatchLoss: G:0.946,D:1.411\n",
      "Batch:18/88 BatchLoss: G:0.951,D:1.428\n",
      "Batch:19/88 BatchLoss: G:0.969,D:1.444\n",
      "Batch:20/88 BatchLoss: G:0.867,D:1.460\n",
      "Batch:21/88 BatchLoss: G:0.944,D:1.443\n",
      "Batch:22/88 BatchLoss: G:0.948,D:1.447\n",
      "Batch:23/88 BatchLoss: G:0.944,D:1.443\n",
      "Batch:24/88 BatchLoss: G:0.958,D:1.458\n",
      "Batch:25/88 BatchLoss: G:0.934,D:1.441\n",
      "Batch:26/88 BatchLoss: G:0.950,D:1.442\n",
      "Batch:27/88 BatchLoss: G:0.909,D:1.441\n",
      "Batch:28/88 BatchLoss: G:0.872,D:1.425\n",
      "Batch:29/88 BatchLoss: G:0.911,D:1.432\n",
      "Batch:30/88 BatchLoss: G:0.927,D:1.415\n",
      "Batch:31/88 BatchLoss: G:0.915,D:1.414\n",
      "Batch:32/88 BatchLoss: G:0.926,D:1.412\n",
      "Batch:33/88 BatchLoss: G:0.962,D:1.403\n",
      "Batch:34/88 BatchLoss: G:0.999,D:1.404\n",
      "Batch:35/88 BatchLoss: G:1.016,D:1.397\n",
      "Batch:36/88 BatchLoss: G:1.004,D:1.398\n",
      "Batch:37/88 BatchLoss: G:1.024,D:1.397\n",
      "Batch:38/88 BatchLoss: G:1.035,D:1.395\n",
      "Batch:39/88 BatchLoss: G:0.978,D:1.392\n",
      "Batch:40/88 BatchLoss: G:0.956,D:1.376\n",
      "Batch:41/88 BatchLoss: G:1.008,D:1.383\n",
      "Batch:42/88 BatchLoss: G:1.023,D:1.390\n",
      "Batch:43/88 BatchLoss: G:0.974,D:1.388\n",
      "Batch:44/88 BatchLoss: G:0.913,D:1.387\n",
      "Batch:45/88 BatchLoss: G:0.962,D:1.397\n",
      "Batch:46/88 BatchLoss: G:0.890,D:1.379\n",
      "Batch:47/88 BatchLoss: G:0.956,D:1.386\n",
      "Batch:48/88 BatchLoss: G:0.890,D:1.378\n",
      "Batch:49/88 BatchLoss: G:0.868,D:1.405\n",
      "Batch:50/88 BatchLoss: G:0.940,D:1.392\n",
      "Batch:51/88 BatchLoss: G:0.960,D:1.401\n",
      "Batch:52/88 BatchLoss: G:0.957,D:1.386\n",
      "Batch:53/88 BatchLoss: G:0.966,D:1.393\n",
      "Batch:54/88 BatchLoss: G:0.936,D:1.390\n",
      "Batch:55/88 BatchLoss: G:1.004,D:1.394\n",
      "Batch:56/88 BatchLoss: G:0.968,D:1.395\n",
      "Batch:57/88 BatchLoss: G:0.883,D:1.394\n",
      "Batch:58/88 BatchLoss: G:0.952,D:1.384\n",
      "Batch:59/88 BatchLoss: G:0.878,D:1.393\n",
      "Batch:60/88 BatchLoss: G:0.972,D:1.383\n",
      "Batch:61/88 BatchLoss: G:0.921,D:1.383\n",
      "Batch:62/88 BatchLoss: G:0.918,D:1.391\n",
      "Batch:63/88 BatchLoss: G:0.890,D:1.390\n",
      "Batch:64/88 BatchLoss: G:0.870,D:1.387\n",
      "Batch:65/88 BatchLoss: G:0.977,D:1.388\n",
      "Batch:66/88 BatchLoss: G:0.865,D:1.394\n",
      "Batch:67/88 BatchLoss: G:0.971,D:1.380\n",
      "Batch:68/88 BatchLoss: G:0.963,D:1.387\n",
      "Batch:69/88 BatchLoss: G:0.959,D:1.380\n",
      "Batch:70/88 BatchLoss: G:0.905,D:1.372\n",
      "Batch:71/88 BatchLoss: G:0.986,D:1.392\n",
      "Batch:72/88 BatchLoss: G:0.936,D:1.380\n",
      "Batch:73/88 BatchLoss: G:0.862,D:1.381\n",
      "Batch:74/88 BatchLoss: G:0.931,D:1.388\n",
      "Batch:75/88 BatchLoss: G:0.948,D:1.383\n",
      "Batch:76/88 BatchLoss: G:0.921,D:1.378\n",
      "Batch:77/88 BatchLoss: G:0.974,D:1.387\n",
      "Batch:78/88 BatchLoss: G:0.961,D:1.386\n",
      "Batch:79/88 BatchLoss: G:0.896,D:1.387\n",
      "Batch:80/88 BatchLoss: G:0.898,D:1.383\n",
      "Batch:81/88 BatchLoss: G:0.910,D:1.381\n",
      "Batch:82/88 BatchLoss: G:0.896,D:1.379\n",
      "Batch:83/88 BatchLoss: G:0.889,D:1.386\n",
      "Batch:84/88 BatchLoss: G:0.928,D:1.385\n",
      "Batch:85/88 BatchLoss: G:0.876,D:1.377\n",
      "Batch:86/88 BatchLoss: G:0.952,D:1.386\n",
      "Batch:87/88 BatchLoss: G:0.879,D:1.375\n",
      "EpochLoss:G:85.29472351074219,D:123.2548599243164\n",
      "Batch:0/88 BatchLoss: G:1.008,D:1.390\n",
      "Batch:1/88 BatchLoss: G:0.923,D:1.381\n",
      "Batch:2/88 BatchLoss: G:0.958,D:1.379\n",
      "Batch:3/88 BatchLoss: G:1.058,D:1.385\n",
      "Batch:4/88 BatchLoss: G:1.105,D:1.403\n",
      "Batch:5/88 BatchLoss: G:1.009,D:1.399\n",
      "Batch:6/88 BatchLoss: G:0.981,D:1.379\n",
      "Batch:7/88 BatchLoss: G:0.925,D:1.374\n",
      "Batch:8/88 BatchLoss: G:0.906,D:1.383\n",
      "Batch:9/88 BatchLoss: G:0.875,D:1.381\n",
      "Batch:10/88 BatchLoss: G:0.821,D:1.388\n",
      "Batch:11/88 BatchLoss: G:0.926,D:1.375\n",
      "Batch:12/88 BatchLoss: G:0.981,D:1.388\n",
      "Batch:13/88 BatchLoss: G:0.976,D:1.374\n",
      "Batch:14/88 BatchLoss: G:0.952,D:1.415\n",
      "Batch:15/88 BatchLoss: G:0.873,D:1.393\n",
      "Batch:16/88 BatchLoss: G:0.824,D:1.385\n",
      "Batch:17/88 BatchLoss: G:0.875,D:1.385\n",
      "Batch:18/88 BatchLoss: G:0.876,D:1.382\n",
      "Batch:19/88 BatchLoss: G:0.890,D:1.383\n",
      "Batch:20/88 BatchLoss: G:0.902,D:1.381\n",
      "Batch:21/88 BatchLoss: G:0.995,D:1.384\n",
      "Batch:22/88 BatchLoss: G:0.978,D:1.372\n",
      "Batch:23/88 BatchLoss: G:0.990,D:1.406\n",
      "Batch:24/88 BatchLoss: G:0.983,D:1.402\n",
      "Batch:25/88 BatchLoss: G:0.924,D:1.388\n",
      "Batch:26/88 BatchLoss: G:0.890,D:1.392\n",
      "Batch:27/88 BatchLoss: G:0.903,D:1.395\n",
      "Batch:28/88 BatchLoss: G:0.901,D:1.385\n",
      "Batch:29/88 BatchLoss: G:0.797,D:1.399\n",
      "Batch:30/88 BatchLoss: G:0.791,D:1.404\n",
      "Batch:31/88 BatchLoss: G:0.854,D:1.395\n",
      "Batch:32/88 BatchLoss: G:0.904,D:1.377\n",
      "Batch:33/88 BatchLoss: G:0.927,D:1.382\n",
      "Batch:34/88 BatchLoss: G:0.999,D:1.387\n",
      "Batch:35/88 BatchLoss: G:1.054,D:1.381\n",
      "Batch:36/88 BatchLoss: G:1.075,D:1.375\n",
      "Batch:37/88 BatchLoss: G:0.985,D:1.379\n",
      "Batch:38/88 BatchLoss: G:1.086,D:1.430\n",
      "Batch:39/88 BatchLoss: G:0.926,D:1.390\n",
      "Batch:40/88 BatchLoss: G:0.877,D:1.376\n",
      "Batch:41/88 BatchLoss: G:0.827,D:1.384\n",
      "Batch:42/88 BatchLoss: G:0.830,D:1.384\n",
      "Batch:43/88 BatchLoss: G:0.849,D:1.383\n",
      "Batch:44/88 BatchLoss: G:0.875,D:1.380\n",
      "Batch:45/88 BatchLoss: G:0.862,D:1.388\n",
      "Batch:46/88 BatchLoss: G:0.915,D:1.388\n",
      "Batch:47/88 BatchLoss: G:0.872,D:1.390\n",
      "Batch:48/88 BatchLoss: G:0.994,D:1.374\n",
      "Batch:49/88 BatchLoss: G:1.088,D:1.358\n",
      "Batch:50/88 BatchLoss: G:1.024,D:1.409\n",
      "Batch:51/88 BatchLoss: G:0.949,D:1.393\n",
      "Batch:52/88 BatchLoss: G:0.916,D:1.375\n",
      "Batch:53/88 BatchLoss: G:0.859,D:1.377\n",
      "Batch:54/88 BatchLoss: G:0.852,D:1.400\n",
      "Batch:55/88 BatchLoss: G:0.799,D:1.381\n",
      "Batch:56/88 BatchLoss: G:0.812,D:1.395\n",
      "Batch:57/88 BatchLoss: G:0.820,D:1.389\n",
      "Batch:58/88 BatchLoss: G:0.868,D:1.394\n",
      "Batch:59/88 BatchLoss: G:0.857,D:1.388\n",
      "Batch:60/88 BatchLoss: G:0.966,D:1.378\n",
      "Batch:61/88 BatchLoss: G:1.024,D:1.382\n",
      "Batch:62/88 BatchLoss: G:1.110,D:1.392\n",
      "Batch:63/88 BatchLoss: G:1.077,D:1.420\n",
      "Batch:64/88 BatchLoss: G:0.954,D:1.381\n",
      "Batch:65/88 BatchLoss: G:0.875,D:1.366\n",
      "Batch:66/88 BatchLoss: G:0.882,D:1.372\n",
      "Batch:67/88 BatchLoss: G:0.876,D:1.383\n",
      "Batch:68/88 BatchLoss: G:0.856,D:1.388\n",
      "Batch:69/88 BatchLoss: G:0.892,D:1.365\n",
      "Batch:70/88 BatchLoss: G:0.922,D:1.379\n",
      "Batch:71/88 BatchLoss: G:0.927,D:1.370\n",
      "Batch:72/88 BatchLoss: G:0.996,D:1.377\n",
      "Batch:73/88 BatchLoss: G:0.890,D:1.382\n",
      "Batch:74/88 BatchLoss: G:0.908,D:1.387\n",
      "Batch:75/88 BatchLoss: G:0.866,D:1.379\n",
      "Batch:76/88 BatchLoss: G:0.855,D:1.388\n",
      "Batch:77/88 BatchLoss: G:0.892,D:1.399\n",
      "Batch:78/88 BatchLoss: G:0.798,D:1.392\n",
      "Batch:79/88 BatchLoss: G:0.898,D:1.398\n",
      "Batch:80/88 BatchLoss: G:0.916,D:1.383\n",
      "Batch:81/88 BatchLoss: G:0.984,D:1.371\n",
      "Batch:82/88 BatchLoss: G:1.001,D:1.394\n",
      "Batch:83/88 BatchLoss: G:0.970,D:1.387\n",
      "Batch:84/88 BatchLoss: G:0.892,D:1.426\n",
      "Batch:85/88 BatchLoss: G:0.823,D:1.394\n",
      "Batch:86/88 BatchLoss: G:0.805,D:1.397\n",
      "Batch:87/88 BatchLoss: G:0.735,D:1.415\n",
      "EpochLoss:G:80.93998718261719,D:122.08218383789062\n",
      "Batch:0/88 BatchLoss: G:0.799,D:1.405\n",
      "Batch:1/88 BatchLoss: G:0.858,D:1.386\n",
      "Batch:2/88 BatchLoss: G:0.845,D:1.390\n",
      "Batch:3/88 BatchLoss: G:0.949,D:1.387\n",
      "Batch:4/88 BatchLoss: G:0.978,D:1.384\n",
      "Batch:5/88 BatchLoss: G:0.998,D:1.393\n",
      "Batch:6/88 BatchLoss: G:1.014,D:1.387\n",
      "Batch:7/88 BatchLoss: G:1.015,D:1.411\n",
      "Batch:8/88 BatchLoss: G:0.925,D:1.388\n",
      "Batch:9/88 BatchLoss: G:0.929,D:1.384\n",
      "Batch:10/88 BatchLoss: G:0.822,D:1.381\n",
      "Batch:11/88 BatchLoss: G:0.800,D:1.393\n",
      "Batch:12/88 BatchLoss: G:0.840,D:1.378\n",
      "Batch:13/88 BatchLoss: G:0.903,D:1.376\n",
      "Batch:14/88 BatchLoss: G:0.944,D:1.387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:15/88 BatchLoss: G:0.912,D:1.389\n",
      "Batch:16/88 BatchLoss: G:0.946,D:1.382\n",
      "Batch:17/88 BatchLoss: G:0.895,D:1.378\n",
      "Batch:18/88 BatchLoss: G:0.861,D:1.373\n",
      "Batch:19/88 BatchLoss: G:0.945,D:1.399\n",
      "Batch:20/88 BatchLoss: G:0.846,D:1.374\n",
      "Batch:21/88 BatchLoss: G:0.845,D:1.383\n",
      "Batch:22/88 BatchLoss: G:0.905,D:1.379\n",
      "Batch:23/88 BatchLoss: G:0.882,D:1.400\n",
      "Batch:24/88 BatchLoss: G:0.850,D:1.386\n",
      "Batch:25/88 BatchLoss: G:0.897,D:1.411\n",
      "Batch:26/88 BatchLoss: G:0.859,D:1.389\n",
      "Batch:27/88 BatchLoss: G:0.852,D:1.388\n",
      "Batch:28/88 BatchLoss: G:0.852,D:1.389\n",
      "Batch:29/88 BatchLoss: G:0.877,D:1.388\n",
      "Batch:30/88 BatchLoss: G:0.872,D:1.393\n",
      "Batch:31/88 BatchLoss: G:0.941,D:1.375\n",
      "Batch:32/88 BatchLoss: G:0.897,D:1.384\n",
      "Batch:33/88 BatchLoss: G:0.917,D:1.386\n",
      "Batch:34/88 BatchLoss: G:0.981,D:1.418\n",
      "Batch:35/88 BatchLoss: G:0.885,D:1.395\n",
      "Batch:36/88 BatchLoss: G:0.801,D:1.393\n",
      "Batch:37/88 BatchLoss: G:0.785,D:1.387\n",
      "Batch:38/88 BatchLoss: G:0.865,D:1.380\n",
      "Batch:39/88 BatchLoss: G:0.857,D:1.380\n",
      "Batch:40/88 BatchLoss: G:0.972,D:1.376\n",
      "Batch:41/88 BatchLoss: G:0.968,D:1.347\n",
      "Batch:42/88 BatchLoss: G:0.972,D:1.362\n",
      "Batch:43/88 BatchLoss: G:0.933,D:1.381\n",
      "Batch:44/88 BatchLoss: G:0.921,D:1.423\n",
      "Batch:45/88 BatchLoss: G:0.839,D:1.398\n",
      "Batch:46/88 BatchLoss: G:0.812,D:1.393\n",
      "Batch:47/88 BatchLoss: G:0.821,D:1.391\n",
      "Batch:48/88 BatchLoss: G:0.930,D:1.377\n",
      "Batch:49/88 BatchLoss: G:1.004,D:1.373\n",
      "Batch:50/88 BatchLoss: G:1.001,D:1.427\n",
      "Batch:51/88 BatchLoss: G:0.831,D:1.380\n",
      "Batch:52/88 BatchLoss: G:0.773,D:1.384\n",
      "Batch:53/88 BatchLoss: G:0.770,D:1.396\n",
      "Batch:54/88 BatchLoss: G:0.780,D:1.412\n",
      "Batch:55/88 BatchLoss: G:0.922,D:1.380\n",
      "Batch:56/88 BatchLoss: G:0.964,D:1.393\n",
      "Batch:57/88 BatchLoss: G:0.924,D:1.393\n",
      "Batch:58/88 BatchLoss: G:0.893,D:1.381\n",
      "Batch:59/88 BatchLoss: G:0.888,D:1.384\n",
      "Batch:60/88 BatchLoss: G:0.823,D:1.386\n",
      "Batch:61/88 BatchLoss: G:0.898,D:1.395\n",
      "Batch:62/88 BatchLoss: G:0.848,D:1.380\n",
      "Batch:63/88 BatchLoss: G:0.798,D:1.388\n",
      "Batch:64/88 BatchLoss: G:0.868,D:1.378\n",
      "Batch:65/88 BatchLoss: G:0.942,D:1.376\n",
      "Batch:66/88 BatchLoss: G:0.956,D:1.383\n",
      "Batch:67/88 BatchLoss: G:0.992,D:1.361\n",
      "Batch:68/88 BatchLoss: G:1.012,D:1.387\n",
      "Batch:69/88 BatchLoss: G:0.969,D:1.400\n",
      "Batch:70/88 BatchLoss: G:0.868,D:1.383\n",
      "Batch:71/88 BatchLoss: G:0.869,D:1.385\n",
      "Batch:72/88 BatchLoss: G:0.795,D:1.372\n",
      "Batch:73/88 BatchLoss: G:0.812,D:1.389\n",
      "Batch:74/88 BatchLoss: G:0.809,D:1.367\n",
      "Batch:75/88 BatchLoss: G:0.858,D:1.380\n",
      "Batch:76/88 BatchLoss: G:0.876,D:1.376\n",
      "Batch:77/88 BatchLoss: G:0.863,D:1.368\n",
      "Batch:78/88 BatchLoss: G:0.917,D:1.382\n",
      "Batch:79/88 BatchLoss: G:0.958,D:1.393\n",
      "Batch:80/88 BatchLoss: G:0.935,D:1.422\n",
      "Batch:81/88 BatchLoss: G:0.833,D:1.398\n",
      "Batch:82/88 BatchLoss: G:0.777,D:1.377\n",
      "Batch:83/88 BatchLoss: G:0.807,D:1.393\n",
      "Batch:84/88 BatchLoss: G:0.889,D:1.408\n",
      "Batch:85/88 BatchLoss: G:0.899,D:1.408\n",
      "Batch:86/88 BatchLoss: G:0.898,D:1.380\n",
      "Batch:87/88 BatchLoss: G:0.912,D:1.371\n",
      "EpochLoss:G:78.24795532226562,D:122.06610107421875\n",
      "Batch:0/88 BatchLoss: G:0.929,D:1.379\n",
      "Batch:1/88 BatchLoss: G:0.878,D:1.392\n",
      "Batch:2/88 BatchLoss: G:0.822,D:1.380\n",
      "Batch:3/88 BatchLoss: G:0.837,D:1.379\n",
      "Batch:4/88 BatchLoss: G:0.839,D:1.373\n",
      "Batch:5/88 BatchLoss: G:0.898,D:1.394\n",
      "Batch:6/88 BatchLoss: G:0.856,D:1.381\n",
      "Batch:7/88 BatchLoss: G:0.834,D:1.357\n",
      "Batch:8/88 BatchLoss: G:0.855,D:1.367\n",
      "Batch:9/88 BatchLoss: G:0.938,D:1.379\n",
      "Batch:10/88 BatchLoss: G:0.942,D:1.358\n",
      "Batch:11/88 BatchLoss: G:0.901,D:1.374\n",
      "Batch:12/88 BatchLoss: G:0.862,D:1.418\n",
      "Batch:13/88 BatchLoss: G:0.772,D:1.382\n",
      "Batch:14/88 BatchLoss: G:0.839,D:1.393\n",
      "Batch:15/88 BatchLoss: G:0.849,D:1.396\n",
      "Batch:16/88 BatchLoss: G:0.943,D:1.395\n",
      "Batch:17/88 BatchLoss: G:0.964,D:1.399\n",
      "Batch:18/88 BatchLoss: G:0.818,D:1.395\n",
      "Batch:19/88 BatchLoss: G:0.782,D:1.387\n",
      "Batch:20/88 BatchLoss: G:0.793,D:1.389\n",
      "Batch:21/88 BatchLoss: G:0.858,D:1.396\n",
      "Batch:22/88 BatchLoss: G:0.926,D:1.378\n",
      "Batch:23/88 BatchLoss: G:0.893,D:1.382\n",
      "Batch:24/88 BatchLoss: G:0.855,D:1.377\n",
      "Batch:25/88 BatchLoss: G:0.839,D:1.380\n",
      "Batch:26/88 BatchLoss: G:0.817,D:1.378\n",
      "Batch:27/88 BatchLoss: G:0.839,D:1.371\n",
      "Batch:28/88 BatchLoss: G:0.878,D:1.379\n",
      "Batch:29/88 BatchLoss: G:0.838,D:1.373\n",
      "Batch:30/88 BatchLoss: G:0.870,D:1.382\n",
      "Batch:31/88 BatchLoss: G:0.869,D:1.373\n",
      "Batch:32/88 BatchLoss: G:0.853,D:1.377\n",
      "Batch:33/88 BatchLoss: G:0.883,D:1.391\n",
      "Batch:34/88 BatchLoss: G:0.813,D:1.383\n",
      "Batch:35/88 BatchLoss: G:0.870,D:1.379\n",
      "Batch:36/88 BatchLoss: G:0.921,D:1.364\n",
      "Batch:37/88 BatchLoss: G:0.943,D:1.398\n",
      "Batch:38/88 BatchLoss: G:0.841,D:1.384\n",
      "Batch:39/88 BatchLoss: G:0.790,D:1.391\n",
      "Batch:40/88 BatchLoss: G:0.827,D:1.387\n",
      "Batch:41/88 BatchLoss: G:0.848,D:1.387\n",
      "Batch:42/88 BatchLoss: G:0.886,D:1.384\n",
      "Batch:43/88 BatchLoss: G:0.855,D:1.383\n",
      "Batch:44/88 BatchLoss: G:0.806,D:1.382\n",
      "Batch:45/88 BatchLoss: G:0.798,D:1.383\n",
      "Batch:46/88 BatchLoss: G:0.865,D:1.379\n",
      "Batch:47/88 BatchLoss: G:0.908,D:1.381\n",
      "Batch:48/88 BatchLoss: G:0.893,D:1.379\n",
      "Batch:49/88 BatchLoss: G:0.848,D:1.396\n",
      "Batch:50/88 BatchLoss: G:0.874,D:1.375\n",
      "Batch:51/88 BatchLoss: G:0.827,D:1.392\n",
      "Batch:52/88 BatchLoss: G:0.817,D:1.388\n",
      "Batch:53/88 BatchLoss: G:0.849,D:1.386\n",
      "Batch:54/88 BatchLoss: G:0.967,D:1.393\n",
      "Batch:55/88 BatchLoss: G:0.926,D:1.406\n",
      "Batch:56/88 BatchLoss: G:0.791,D:1.381\n",
      "Batch:57/88 BatchLoss: G:0.745,D:1.380\n",
      "Batch:58/88 BatchLoss: G:0.789,D:1.407\n",
      "Batch:59/88 BatchLoss: G:0.914,D:1.391\n",
      "Batch:60/88 BatchLoss: G:0.994,D:1.388\n",
      "Batch:61/88 BatchLoss: G:0.952,D:1.429\n",
      "Batch:62/88 BatchLoss: G:0.799,D:1.418\n",
      "Batch:63/88 BatchLoss: G:0.753,D:1.447\n",
      "Batch:64/88 BatchLoss: G:0.776,D:1.413\n",
      "Batch:65/88 BatchLoss: G:0.899,D:1.394\n",
      "Batch:66/88 BatchLoss: G:0.928,D:1.374\n",
      "Batch:67/88 BatchLoss: G:0.943,D:1.376\n",
      "Batch:68/88 BatchLoss: G:0.902,D:1.383\n",
      "Batch:69/88 BatchLoss: G:0.864,D:1.384\n",
      "Batch:70/88 BatchLoss: G:0.823,D:1.384\n",
      "Batch:71/88 BatchLoss: G:0.789,D:1.389\n",
      "Batch:72/88 BatchLoss: G:0.890,D:1.392\n",
      "Batch:73/88 BatchLoss: G:0.856,D:1.381\n",
      "Batch:74/88 BatchLoss: G:0.913,D:1.384\n",
      "Batch:75/88 BatchLoss: G:0.938,D:1.404\n",
      "Batch:76/88 BatchLoss: G:0.845,D:1.380\n",
      "Batch:77/88 BatchLoss: G:0.789,D:1.389\n",
      "Batch:78/88 BatchLoss: G:0.851,D:1.383\n",
      "Batch:79/88 BatchLoss: G:0.866,D:1.383\n",
      "Batch:80/88 BatchLoss: G:0.879,D:1.381\n",
      "Batch:81/88 BatchLoss: G:0.922,D:1.385\n",
      "Batch:82/88 BatchLoss: G:0.865,D:1.380\n",
      "Batch:83/88 BatchLoss: G:0.867,D:1.384\n",
      "Batch:84/88 BatchLoss: G:0.817,D:1.383\n",
      "Batch:85/88 BatchLoss: G:0.835,D:1.378\n",
      "Batch:86/88 BatchLoss: G:0.808,D:1.387\n",
      "Batch:87/88 BatchLoss: G:0.813,D:1.394\n",
      "EpochLoss:G:75.75588989257812,D:121.99073791503906\n",
      "Batch:0/88 BatchLoss: G:0.884,D:1.381\n",
      "Batch:1/88 BatchLoss: G:0.869,D:1.387\n",
      "Batch:2/88 BatchLoss: G:0.827,D:1.390\n",
      "Batch:3/88 BatchLoss: G:0.809,D:1.386\n",
      "Batch:4/88 BatchLoss: G:0.806,D:1.385\n",
      "Batch:5/88 BatchLoss: G:0.826,D:1.383\n",
      "Batch:6/88 BatchLoss: G:0.856,D:1.373\n",
      "Batch:7/88 BatchLoss: G:0.894,D:1.371\n",
      "Batch:8/88 BatchLoss: G:0.929,D:1.385\n",
      "Batch:9/88 BatchLoss: G:0.879,D:1.380\n",
      "Batch:10/88 BatchLoss: G:0.843,D:1.383\n",
      "Batch:11/88 BatchLoss: G:0.819,D:1.385\n",
      "Batch:12/88 BatchLoss: G:0.785,D:1.383\n",
      "Batch:13/88 BatchLoss: G:0.842,D:1.381\n",
      "Batch:14/88 BatchLoss: G:0.875,D:1.393\n",
      "Batch:15/88 BatchLoss: G:0.849,D:1.393\n",
      "Batch:16/88 BatchLoss: G:0.874,D:1.392\n",
      "Batch:17/88 BatchLoss: G:0.840,D:1.391\n",
      "Batch:18/88 BatchLoss: G:0.819,D:1.388\n",
      "Batch:19/88 BatchLoss: G:0.865,D:1.386\n",
      "Batch:20/88 BatchLoss: G:0.851,D:1.384\n",
      "Batch:21/88 BatchLoss: G:0.842,D:1.380\n",
      "Batch:22/88 BatchLoss: G:0.840,D:1.388\n",
      "Batch:23/88 BatchLoss: G:0.838,D:1.380\n",
      "Batch:24/88 BatchLoss: G:0.889,D:1.377\n",
      "Batch:25/88 BatchLoss: G:0.880,D:1.376\n",
      "Batch:26/88 BatchLoss: G:0.815,D:1.377\n",
      "Batch:27/88 BatchLoss: G:0.771,D:1.375\n",
      "Batch:28/88 BatchLoss: G:0.778,D:1.381\n",
      "Batch:29/88 BatchLoss: G:0.867,D:1.384\n",
      "Batch:30/88 BatchLoss: G:0.929,D:1.386\n",
      "Batch:31/88 BatchLoss: G:0.888,D:1.399\n",
      "Batch:32/88 BatchLoss: G:0.794,D:1.397\n",
      "Batch:33/88 BatchLoss: G:0.750,D:1.394\n",
      "Batch:34/88 BatchLoss: G:0.770,D:1.413\n",
      "Batch:35/88 BatchLoss: G:0.907,D:1.388\n",
      "Batch:36/88 BatchLoss: G:0.919,D:1.396\n",
      "Batch:37/88 BatchLoss: G:0.905,D:1.376\n",
      "Batch:38/88 BatchLoss: G:0.858,D:1.387\n",
      "Batch:39/88 BatchLoss: G:0.818,D:1.370\n",
      "Batch:40/88 BatchLoss: G:0.803,D:1.367\n",
      "Batch:41/88 BatchLoss: G:0.882,D:1.379\n",
      "Batch:42/88 BatchLoss: G:0.858,D:1.377\n",
      "Batch:43/88 BatchLoss: G:0.874,D:1.374\n",
      "Batch:44/88 BatchLoss: G:0.897,D:1.356\n",
      "Batch:45/88 BatchLoss: G:0.890,D:1.369\n",
      "Batch:46/88 BatchLoss: G:0.897,D:1.378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:47/88 BatchLoss: G:0.813,D:1.380\n",
      "Batch:48/88 BatchLoss: G:0.816,D:1.373\n",
      "Batch:49/88 BatchLoss: G:0.856,D:1.378\n",
      "Batch:50/88 BatchLoss: G:0.824,D:1.381\n",
      "Batch:51/88 BatchLoss: G:0.849,D:1.377\n",
      "Batch:52/88 BatchLoss: G:0.847,D:1.384\n",
      "Batch:53/88 BatchLoss: G:0.850,D:1.383\n",
      "Batch:54/88 BatchLoss: G:0.817,D:1.384\n",
      "Batch:55/88 BatchLoss: G:0.787,D:1.378\n",
      "Batch:56/88 BatchLoss: G:0.825,D:1.387\n",
      "Batch:57/88 BatchLoss: G:0.847,D:1.387\n",
      "Batch:58/88 BatchLoss: G:0.922,D:1.375\n",
      "Batch:59/88 BatchLoss: G:0.908,D:1.386\n",
      "Batch:60/88 BatchLoss: G:0.833,D:1.380\n",
      "Batch:61/88 BatchLoss: G:0.824,D:1.383\n",
      "Batch:62/88 BatchLoss: G:0.826,D:1.381\n",
      "Batch:63/88 BatchLoss: G:0.815,D:1.373\n",
      "Batch:64/88 BatchLoss: G:0.869,D:1.376\n",
      "Batch:65/88 BatchLoss: G:0.890,D:1.374\n",
      "Batch:66/88 BatchLoss: G:0.862,D:1.381\n",
      "Batch:67/88 BatchLoss: G:0.810,D:1.371\n",
      "Batch:68/88 BatchLoss: G:0.830,D:1.382\n",
      "Batch:69/88 BatchLoss: G:0.823,D:1.385\n",
      "Batch:70/88 BatchLoss: G:0.862,D:1.369\n",
      "Batch:71/88 BatchLoss: G:0.883,D:1.374\n",
      "Batch:72/88 BatchLoss: G:0.820,D:1.369\n",
      "Batch:73/88 BatchLoss: G:0.816,D:1.371\n",
      "Batch:74/88 BatchLoss: G:0.861,D:1.363\n",
      "Batch:75/88 BatchLoss: G:0.859,D:1.358\n",
      "Batch:76/88 BatchLoss: G:0.885,D:1.372\n",
      "Batch:77/88 BatchLoss: G:0.873,D:1.381\n",
      "Batch:78/88 BatchLoss: G:0.779,D:1.369\n",
      "Batch:79/88 BatchLoss: G:0.826,D:1.370\n",
      "Batch:80/88 BatchLoss: G:0.862,D:1.373\n",
      "Batch:81/88 BatchLoss: G:0.884,D:1.360\n",
      "Batch:82/88 BatchLoss: G:0.878,D:1.390\n",
      "Batch:83/88 BatchLoss: G:0.811,D:1.370\n",
      "Batch:84/88 BatchLoss: G:0.810,D:1.381\n",
      "Batch:85/88 BatchLoss: G:0.857,D:1.394\n",
      "Batch:86/88 BatchLoss: G:0.871,D:1.386\n",
      "Batch:87/88 BatchLoss: G:0.780,D:1.385\n",
      "EpochLoss:G:74.56334686279297,D:121.47925567626953\n",
      "Batch:0/88 BatchLoss: G:0.755,D:1.391\n",
      "Batch:1/88 BatchLoss: G:0.883,D:1.396\n",
      "Batch:2/88 BatchLoss: G:0.935,D:1.389\n",
      "Batch:3/88 BatchLoss: G:0.862,D:1.397\n",
      "Batch:4/88 BatchLoss: G:0.777,D:1.388\n",
      "Batch:5/88 BatchLoss: G:0.790,D:1.399\n",
      "Batch:6/88 BatchLoss: G:0.911,D:1.392\n",
      "Batch:7/88 BatchLoss: G:0.961,D:1.398\n",
      "Batch:8/88 BatchLoss: G:0.861,D:1.386\n",
      "Batch:9/88 BatchLoss: G:0.801,D:1.379\n",
      "Batch:10/88 BatchLoss: G:0.815,D:1.387\n",
      "Batch:11/88 BatchLoss: G:0.828,D:1.383\n",
      "Batch:12/88 BatchLoss: G:0.803,D:1.391\n",
      "Batch:13/88 BatchLoss: G:0.851,D:1.386\n",
      "Batch:14/88 BatchLoss: G:0.860,D:1.383\n",
      "Batch:15/88 BatchLoss: G:0.847,D:1.385\n",
      "Batch:16/88 BatchLoss: G:0.800,D:1.385\n",
      "Batch:17/88 BatchLoss: G:0.866,D:1.379\n",
      "Batch:18/88 BatchLoss: G:0.859,D:1.378\n",
      "Batch:19/88 BatchLoss: G:0.893,D:1.369\n",
      "Batch:20/88 BatchLoss: G:0.887,D:1.379\n",
      "Batch:21/88 BatchLoss: G:0.833,D:1.363\n",
      "Batch:22/88 BatchLoss: G:0.851,D:1.367\n",
      "Batch:23/88 BatchLoss: G:0.841,D:1.367\n",
      "Batch:24/88 BatchLoss: G:0.862,D:1.362\n",
      "Batch:25/88 BatchLoss: G:0.873,D:1.366\n",
      "Batch:26/88 BatchLoss: G:0.880,D:1.373\n",
      "Batch:27/88 BatchLoss: G:0.861,D:1.368\n",
      "Batch:28/88 BatchLoss: G:0.865,D:1.379\n",
      "Batch:29/88 BatchLoss: G:0.852,D:1.377\n",
      "Batch:30/88 BatchLoss: G:0.835,D:1.369\n",
      "Batch:31/88 BatchLoss: G:0.797,D:1.371\n",
      "Batch:32/88 BatchLoss: G:0.814,D:1.388\n",
      "Batch:33/88 BatchLoss: G:0.903,D:1.376\n",
      "Batch:34/88 BatchLoss: G:0.882,D:1.387\n",
      "Batch:35/88 BatchLoss: G:0.741,D:1.379\n",
      "Batch:36/88 BatchLoss: G:0.767,D:1.378\n",
      "Batch:37/88 BatchLoss: G:0.839,D:1.386\n",
      "Batch:38/88 BatchLoss: G:0.937,D:1.380\n",
      "Batch:39/88 BatchLoss: G:0.926,D:1.373\n",
      "Batch:40/88 BatchLoss: G:0.851,D:1.382\n",
      "Batch:41/88 BatchLoss: G:0.783,D:1.361\n",
      "Batch:42/88 BatchLoss: G:0.788,D:1.385\n",
      "Batch:43/88 BatchLoss: G:0.846,D:1.392\n",
      "Batch:44/88 BatchLoss: G:0.884,D:1.391\n",
      "Batch:45/88 BatchLoss: G:0.880,D:1.397\n",
      "Batch:46/88 BatchLoss: G:0.855,D:1.391\n",
      "Batch:47/88 BatchLoss: G:0.813,D:1.385\n",
      "Batch:48/88 BatchLoss: G:0.842,D:1.374\n",
      "Batch:49/88 BatchLoss: G:0.856,D:1.374\n",
      "Batch:50/88 BatchLoss: G:0.810,D:1.379\n",
      "Batch:51/88 BatchLoss: G:0.824,D:1.381\n",
      "Batch:52/88 BatchLoss: G:0.867,D:1.394\n",
      "Batch:53/88 BatchLoss: G:0.897,D:1.384\n",
      "Batch:54/88 BatchLoss: G:0.870,D:1.408\n",
      "Batch:55/88 BatchLoss: G:0.756,D:1.371\n",
      "Batch:56/88 BatchLoss: G:0.811,D:1.396\n",
      "Batch:57/88 BatchLoss: G:0.917,D:1.383\n",
      "Batch:58/88 BatchLoss: G:0.951,D:1.376\n",
      "Batch:59/88 BatchLoss: G:0.803,D:1.389\n",
      "Batch:60/88 BatchLoss: G:0.750,D:1.363\n",
      "Batch:61/88 BatchLoss: G:0.827,D:1.387\n",
      "Batch:62/88 BatchLoss: G:0.947,D:1.375\n",
      "Batch:63/88 BatchLoss: G:0.966,D:1.366\n",
      "Batch:64/88 BatchLoss: G:0.882,D:1.352\n",
      "Batch:65/88 BatchLoss: G:0.812,D:1.355\n",
      "Batch:66/88 BatchLoss: G:0.807,D:1.371\n",
      "Batch:67/88 BatchLoss: G:0.873,D:1.364\n",
      "Batch:68/88 BatchLoss: G:1.000,D:1.355\n",
      "Batch:69/88 BatchLoss: G:0.915,D:1.356\n",
      "Batch:70/88 BatchLoss: G:0.847,D:1.350\n",
      "Batch:71/88 BatchLoss: G:0.827,D:1.350\n",
      "Batch:72/88 BatchLoss: G:0.851,D:1.351\n",
      "Batch:73/88 BatchLoss: G:0.935,D:1.340\n",
      "Batch:74/88 BatchLoss: G:0.925,D:1.354\n",
      "Batch:75/88 BatchLoss: G:0.856,D:1.372\n",
      "Batch:76/88 BatchLoss: G:0.800,D:1.381\n",
      "Batch:77/88 BatchLoss: G:0.794,D:1.381\n",
      "Batch:78/88 BatchLoss: G:0.836,D:1.388\n",
      "Batch:79/88 BatchLoss: G:0.855,D:1.396\n",
      "Batch:80/88 BatchLoss: G:0.906,D:1.400\n",
      "Batch:81/88 BatchLoss: G:0.831,D:1.411\n",
      "Batch:82/88 BatchLoss: G:0.740,D:1.403\n",
      "Batch:83/88 BatchLoss: G:0.824,D:1.406\n",
      "Batch:84/88 BatchLoss: G:0.900,D:1.394\n",
      "Batch:85/88 BatchLoss: G:0.887,D:1.423\n",
      "Batch:86/88 BatchLoss: G:0.798,D:1.408\n",
      "Batch:87/88 BatchLoss: G:0.818,D:1.406\n",
      "EpochLoss:G:74.91740417480469,D:121.47254943847656\n",
      "Batch:0/88 BatchLoss: G:0.897,D:1.408\n",
      "Batch:1/88 BatchLoss: G:0.849,D:1.407\n",
      "Batch:2/88 BatchLoss: G:0.811,D:1.400\n",
      "Batch:3/88 BatchLoss: G:0.791,D:1.412\n",
      "Batch:4/88 BatchLoss: G:0.803,D:1.396\n",
      "Batch:5/88 BatchLoss: G:0.913,D:1.392\n",
      "Batch:6/88 BatchLoss: G:0.890,D:1.395\n",
      "Batch:7/88 BatchLoss: G:0.846,D:1.378\n",
      "Batch:8/88 BatchLoss: G:0.808,D:1.384\n",
      "Batch:9/88 BatchLoss: G:0.834,D:1.389\n",
      "Batch:10/88 BatchLoss: G:0.831,D:1.381\n",
      "Batch:11/88 BatchLoss: G:0.878,D:1.381\n",
      "Batch:12/88 BatchLoss: G:0.833,D:1.373\n",
      "Batch:13/88 BatchLoss: G:0.836,D:1.369\n",
      "Batch:14/88 BatchLoss: G:0.835,D:1.348\n",
      "Batch:15/88 BatchLoss: G:0.854,D:1.364\n",
      "Batch:16/88 BatchLoss: G:0.860,D:1.348\n",
      "Batch:17/88 BatchLoss: G:0.839,D:1.373\n",
      "Batch:18/88 BatchLoss: G:0.824,D:1.369\n",
      "Batch:19/88 BatchLoss: G:0.895,D:1.351\n",
      "Batch:20/88 BatchLoss: G:0.933,D:1.370\n",
      "Batch:21/88 BatchLoss: G:0.888,D:1.363\n",
      "Batch:22/88 BatchLoss: G:0.807,D:1.355\n",
      "Batch:23/88 BatchLoss: G:0.794,D:1.362\n",
      "Batch:24/88 BatchLoss: G:0.840,D:1.363\n",
      "Batch:25/88 BatchLoss: G:0.860,D:1.364\n",
      "Batch:26/88 BatchLoss: G:0.797,D:1.359\n",
      "Batch:27/88 BatchLoss: G:0.778,D:1.361\n",
      "Batch:28/88 BatchLoss: G:0.832,D:1.369\n",
      "Batch:29/88 BatchLoss: G:0.894,D:1.374\n",
      "Batch:30/88 BatchLoss: G:0.912,D:1.381\n",
      "Batch:31/88 BatchLoss: G:0.836,D:1.383\n",
      "Batch:32/88 BatchLoss: G:0.798,D:1.373\n",
      "Batch:33/88 BatchLoss: G:0.864,D:1.379\n",
      "Batch:34/88 BatchLoss: G:0.901,D:1.378\n",
      "Batch:35/88 BatchLoss: G:0.833,D:1.368\n",
      "Batch:36/88 BatchLoss: G:0.821,D:1.408\n",
      "Batch:37/88 BatchLoss: G:0.858,D:1.386\n",
      "Batch:38/88 BatchLoss: G:0.837,D:1.391\n",
      "Batch:39/88 BatchLoss: G:0.855,D:1.399\n",
      "Batch:40/88 BatchLoss: G:0.884,D:1.391\n",
      "Batch:41/88 BatchLoss: G:0.931,D:1.411\n",
      "Batch:42/88 BatchLoss: G:0.757,D:1.392\n",
      "Batch:43/88 BatchLoss: G:0.734,D:1.398\n",
      "Batch:44/88 BatchLoss: G:0.891,D:1.423\n",
      "Batch:45/88 BatchLoss: G:0.962,D:1.401\n",
      "Batch:46/88 BatchLoss: G:0.915,D:1.399\n",
      "Batch:47/88 BatchLoss: G:0.817,D:1.436\n",
      "Batch:48/88 BatchLoss: G:0.708,D:1.401\n",
      "Batch:49/88 BatchLoss: G:0.744,D:1.419\n",
      "Batch:50/88 BatchLoss: G:0.868,D:1.403\n",
      "Batch:51/88 BatchLoss: G:0.988,D:1.400\n",
      "Batch:52/88 BatchLoss: G:0.941,D:1.412\n",
      "Batch:53/88 BatchLoss: G:0.839,D:1.396\n",
      "Batch:54/88 BatchLoss: G:0.783,D:1.385\n",
      "Batch:55/88 BatchLoss: G:0.777,D:1.387\n",
      "Batch:56/88 BatchLoss: G:0.770,D:1.383\n",
      "Batch:57/88 BatchLoss: G:0.870,D:1.381\n",
      "Batch:58/88 BatchLoss: G:0.928,D:1.369\n",
      "Batch:59/88 BatchLoss: G:0.906,D:1.372\n",
      "Batch:60/88 BatchLoss: G:0.921,D:1.368\n",
      "Batch:61/88 BatchLoss: G:0.879,D:1.369\n",
      "Batch:62/88 BatchLoss: G:0.879,D:1.380\n",
      "Batch:63/88 BatchLoss: G:0.776,D:1.374\n",
      "Batch:64/88 BatchLoss: G:0.823,D:1.377\n",
      "Batch:65/88 BatchLoss: G:0.912,D:1.384\n",
      "Batch:66/88 BatchLoss: G:0.976,D:1.404\n",
      "Batch:67/88 BatchLoss: G:0.903,D:1.404\n",
      "Batch:68/88 BatchLoss: G:0.824,D:1.396\n",
      "Batch:69/88 BatchLoss: G:0.737,D:1.405\n",
      "Batch:70/88 BatchLoss: G:0.799,D:1.400\n",
      "Batch:71/88 BatchLoss: G:0.841,D:1.401\n",
      "Batch:72/88 BatchLoss: G:0.911,D:1.399\n",
      "Batch:73/88 BatchLoss: G:0.877,D:1.394\n",
      "Batch:74/88 BatchLoss: G:0.864,D:1.390\n",
      "Batch:75/88 BatchLoss: G:0.833,D:1.391\n",
      "Batch:76/88 BatchLoss: G:0.770,D:1.384\n",
      "Batch:77/88 BatchLoss: G:0.786,D:1.385\n",
      "Batch:78/88 BatchLoss: G:0.850,D:1.380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:79/88 BatchLoss: G:0.880,D:1.387\n",
      "Batch:80/88 BatchLoss: G:0.832,D:1.381\n",
      "Batch:81/88 BatchLoss: G:0.829,D:1.378\n",
      "Batch:82/88 BatchLoss: G:0.814,D:1.377\n",
      "Batch:83/88 BatchLoss: G:0.832,D:1.379\n",
      "Batch:84/88 BatchLoss: G:0.852,D:1.380\n",
      "Batch:85/88 BatchLoss: G:0.839,D:1.382\n",
      "Batch:86/88 BatchLoss: G:0.895,D:1.374\n",
      "Batch:87/88 BatchLoss: G:0.916,D:1.420\n",
      "EpochLoss:G:74.7298583984375,D:121.90226745605469\n",
      "Batch:0/88 BatchLoss: G:0.781,D:1.384\n",
      "Batch:1/88 BatchLoss: G:0.764,D:1.388\n",
      "Batch:2/88 BatchLoss: G:0.867,D:1.383\n",
      "Batch:3/88 BatchLoss: G:0.967,D:1.359\n",
      "Batch:4/88 BatchLoss: G:0.915,D:1.393\n",
      "Batch:5/88 BatchLoss: G:0.857,D:1.397\n",
      "Batch:6/88 BatchLoss: G:0.759,D:1.351\n",
      "Batch:7/88 BatchLoss: G:0.761,D:1.365\n",
      "Batch:8/88 BatchLoss: G:0.798,D:1.393\n",
      "Batch:9/88 BatchLoss: G:0.863,D:1.393\n",
      "Batch:10/88 BatchLoss: G:0.938,D:1.377\n",
      "Batch:11/88 BatchLoss: G:0.874,D:1.385\n",
      "Batch:12/88 BatchLoss: G:0.798,D:1.376\n",
      "Batch:13/88 BatchLoss: G:0.769,D:1.390\n",
      "Batch:14/88 BatchLoss: G:0.789,D:1.392\n",
      "Batch:15/88 BatchLoss: G:0.877,D:1.389\n",
      "Batch:16/88 BatchLoss: G:0.892,D:1.389\n",
      "Batch:17/88 BatchLoss: G:0.869,D:1.390\n",
      "Batch:18/88 BatchLoss: G:0.799,D:1.384\n",
      "Batch:19/88 BatchLoss: G:0.752,D:1.395\n",
      "Batch:20/88 BatchLoss: G:0.818,D:1.384\n",
      "Batch:21/88 BatchLoss: G:0.862,D:1.380\n",
      "Batch:22/88 BatchLoss: G:0.876,D:1.389\n",
      "Batch:23/88 BatchLoss: G:0.843,D:1.380\n",
      "Batch:24/88 BatchLoss: G:0.829,D:1.378\n",
      "Batch:25/88 BatchLoss: G:0.817,D:1.377\n",
      "Batch:26/88 BatchLoss: G:0.861,D:1.395\n",
      "Batch:27/88 BatchLoss: G:0.871,D:1.366\n",
      "Batch:28/88 BatchLoss: G:0.918,D:1.373\n",
      "Batch:29/88 BatchLoss: G:0.876,D:1.380\n",
      "Batch:30/88 BatchLoss: G:0.850,D:1.325\n",
      "Batch:31/88 BatchLoss: G:0.914,D:1.313\n",
      "Batch:32/88 BatchLoss: G:0.975,D:1.267\n",
      "Batch:33/88 BatchLoss: G:0.952,D:1.213\n",
      "Batch:34/88 BatchLoss: G:0.961,D:1.208\n",
      "Batch:35/88 BatchLoss: G:0.990,D:1.199\n",
      "Batch:36/88 BatchLoss: G:1.148,D:1.195\n",
      "Batch:37/88 BatchLoss: G:1.078,D:1.239\n",
      "Batch:38/88 BatchLoss: G:0.973,D:1.303\n",
      "Batch:39/88 BatchLoss: G:0.839,D:1.360\n",
      "Batch:40/88 BatchLoss: G:0.784,D:1.346\n",
      "Batch:41/88 BatchLoss: G:0.883,D:1.342\n",
      "Batch:42/88 BatchLoss: G:1.011,D:1.345\n",
      "Batch:43/88 BatchLoss: G:1.021,D:1.368\n",
      "Batch:44/88 BatchLoss: G:0.974,D:1.411\n",
      "Batch:45/88 BatchLoss: G:0.919,D:1.395\n",
      "Batch:46/88 BatchLoss: G:0.961,D:1.363\n",
      "Batch:47/88 BatchLoss: G:0.910,D:1.395\n",
      "Batch:48/88 BatchLoss: G:0.891,D:1.386\n",
      "Batch:49/88 BatchLoss: G:0.833,D:1.412\n",
      "Batch:50/88 BatchLoss: G:0.808,D:1.410\n",
      "Batch:51/88 BatchLoss: G:0.924,D:1.404\n",
      "Batch:52/88 BatchLoss: G:0.927,D:1.415\n",
      "Batch:53/88 BatchLoss: G:0.988,D:1.441\n",
      "Batch:54/88 BatchLoss: G:0.887,D:1.419\n",
      "Batch:55/88 BatchLoss: G:0.921,D:1.420\n",
      "Batch:56/88 BatchLoss: G:0.922,D:1.389\n",
      "Batch:57/88 BatchLoss: G:1.162,D:1.365\n",
      "Batch:58/88 BatchLoss: G:1.051,D:1.384\n",
      "Batch:59/88 BatchLoss: G:0.879,D:1.433\n",
      "Batch:60/88 BatchLoss: G:0.678,D:1.591\n",
      "Batch:61/88 BatchLoss: G:0.709,D:1.442\n",
      "Batch:62/88 BatchLoss: G:0.626,D:1.513\n",
      "Batch:63/88 BatchLoss: G:0.720,D:1.469\n",
      "Batch:64/88 BatchLoss: G:0.811,D:1.419\n",
      "Batch:65/88 BatchLoss: G:0.897,D:1.413\n",
      "Batch:66/88 BatchLoss: G:0.992,D:1.423\n",
      "Batch:67/88 BatchLoss: G:0.995,D:1.434\n",
      "Batch:68/88 BatchLoss: G:0.972,D:1.387\n",
      "Batch:69/88 BatchLoss: G:1.166,D:1.441\n",
      "Batch:70/88 BatchLoss: G:1.152,D:1.440\n",
      "Batch:71/88 BatchLoss: G:1.054,D:1.458\n",
      "Batch:72/88 BatchLoss: G:1.141,D:1.444\n",
      "Batch:73/88 BatchLoss: G:0.966,D:1.452\n",
      "Batch:74/88 BatchLoss: G:0.992,D:1.419\n",
      "Batch:75/88 BatchLoss: G:0.935,D:1.374\n",
      "Batch:76/88 BatchLoss: G:0.819,D:1.383\n",
      "Batch:77/88 BatchLoss: G:0.713,D:1.413\n",
      "Batch:78/88 BatchLoss: G:0.761,D:1.410\n",
      "Batch:79/88 BatchLoss: G:0.821,D:1.418\n",
      "Batch:80/88 BatchLoss: G:0.793,D:1.416\n",
      "Batch:81/88 BatchLoss: G:0.738,D:1.423\n",
      "Batch:82/88 BatchLoss: G:0.960,D:1.400\n",
      "Batch:83/88 BatchLoss: G:1.024,D:1.459\n",
      "Batch:84/88 BatchLoss: G:0.956,D:1.442\n",
      "Batch:85/88 BatchLoss: G:0.824,D:1.418\n",
      "Batch:86/88 BatchLoss: G:0.852,D:1.427\n",
      "Batch:87/88 BatchLoss: G:0.784,D:1.416\n",
      "EpochLoss:G:78.44823455810547,D:122.08539581298828\n",
      "Batch:0/88 BatchLoss: G:0.776,D:1.401\n",
      "Batch:1/88 BatchLoss: G:0.847,D:1.401\n",
      "Batch:2/88 BatchLoss: G:0.680,D:1.423\n",
      "Batch:3/88 BatchLoss: G:0.855,D:1.403\n",
      "Batch:4/88 BatchLoss: G:0.727,D:1.405\n",
      "Batch:5/88 BatchLoss: G:0.732,D:1.411\n",
      "Batch:6/88 BatchLoss: G:0.830,D:1.399\n",
      "Batch:7/88 BatchLoss: G:0.822,D:1.401\n",
      "Batch:8/88 BatchLoss: G:0.855,D:1.400\n",
      "Batch:9/88 BatchLoss: G:0.805,D:1.401\n",
      "Batch:10/88 BatchLoss: G:0.849,D:1.397\n",
      "Batch:11/88 BatchLoss: G:0.816,D:1.396\n",
      "Batch:12/88 BatchLoss: G:0.861,D:1.398\n",
      "Batch:13/88 BatchLoss: G:0.917,D:1.407\n",
      "Batch:14/88 BatchLoss: G:0.842,D:1.397\n",
      "Batch:15/88 BatchLoss: G:0.792,D:1.398\n",
      "Batch:16/88 BatchLoss: G:0.878,D:1.403\n",
      "Batch:17/88 BatchLoss: G:0.829,D:1.394\n",
      "Batch:18/88 BatchLoss: G:0.833,D:1.400\n",
      "Batch:19/88 BatchLoss: G:0.855,D:1.399\n",
      "Batch:20/88 BatchLoss: G:0.837,D:1.397\n",
      "Batch:21/88 BatchLoss: G:0.743,D:1.401\n",
      "Batch:22/88 BatchLoss: G:0.800,D:1.393\n",
      "Batch:23/88 BatchLoss: G:0.829,D:1.396\n",
      "Batch:24/88 BatchLoss: G:0.771,D:1.399\n",
      "Batch:25/88 BatchLoss: G:0.836,D:1.397\n",
      "Batch:26/88 BatchLoss: G:0.828,D:1.402\n",
      "Batch:27/88 BatchLoss: G:0.759,D:1.407\n",
      "Batch:28/88 BatchLoss: G:0.767,D:1.408\n",
      "Batch:29/88 BatchLoss: G:0.791,D:1.398\n",
      "Batch:30/88 BatchLoss: G:0.851,D:1.410\n",
      "Batch:31/88 BatchLoss: G:0.831,D:1.397\n",
      "Batch:32/88 BatchLoss: G:0.872,D:1.403\n",
      "Batch:33/88 BatchLoss: G:0.855,D:1.396\n",
      "Batch:34/88 BatchLoss: G:0.849,D:1.396\n",
      "Batch:35/88 BatchLoss: G:0.876,D:1.399\n",
      "Batch:36/88 BatchLoss: G:0.897,D:1.399\n",
      "Batch:37/88 BatchLoss: G:0.844,D:1.403\n",
      "Batch:38/88 BatchLoss: G:0.926,D:1.436\n",
      "Batch:39/88 BatchLoss: G:0.738,D:1.404\n",
      "Batch:40/88 BatchLoss: G:0.764,D:1.405\n",
      "Batch:41/88 BatchLoss: G:0.772,D:1.404\n",
      "Batch:42/88 BatchLoss: G:0.838,D:1.399\n",
      "Batch:43/88 BatchLoss: G:0.762,D:1.402\n",
      "Batch:44/88 BatchLoss: G:0.703,D:1.413\n",
      "Batch:45/88 BatchLoss: G:0.813,D:1.403\n",
      "Batch:46/88 BatchLoss: G:0.810,D:1.394\n",
      "Batch:47/88 BatchLoss: G:0.787,D:1.397\n",
      "Batch:48/88 BatchLoss: G:0.764,D:1.392\n",
      "Batch:49/88 BatchLoss: G:0.832,D:1.401\n",
      "Batch:50/88 BatchLoss: G:0.873,D:1.405\n",
      "Batch:51/88 BatchLoss: G:0.837,D:1.392\n",
      "Batch:52/88 BatchLoss: G:0.948,D:1.398\n",
      "Batch:53/88 BatchLoss: G:0.918,D:1.379\n",
      "Batch:54/88 BatchLoss: G:0.904,D:1.391\n",
      "Batch:55/88 BatchLoss: G:0.925,D:1.404\n",
      "Batch:56/88 BatchLoss: G:0.883,D:1.402\n",
      "Batch:57/88 BatchLoss: G:0.813,D:1.390\n",
      "Batch:58/88 BatchLoss: G:0.698,D:1.401\n",
      "Batch:59/88 BatchLoss: G:0.701,D:1.399\n",
      "Batch:60/88 BatchLoss: G:0.780,D:1.390\n",
      "Batch:61/88 BatchLoss: G:0.825,D:1.394\n",
      "Batch:62/88 BatchLoss: G:0.817,D:1.396\n",
      "Batch:63/88 BatchLoss: G:0.810,D:1.396\n",
      "Batch:64/88 BatchLoss: G:0.815,D:1.398\n",
      "Batch:65/88 BatchLoss: G:0.845,D:1.395\n",
      "Batch:66/88 BatchLoss: G:0.821,D:1.393\n",
      "Batch:67/88 BatchLoss: G:0.845,D:1.396\n",
      "Batch:68/88 BatchLoss: G:0.833,D:1.401\n",
      "Batch:69/88 BatchLoss: G:0.828,D:1.406\n",
      "Batch:70/88 BatchLoss: G:0.854,D:1.408\n",
      "Batch:71/88 BatchLoss: G:0.771,D:1.406\n",
      "Batch:72/88 BatchLoss: G:0.780,D:1.407\n",
      "Batch:73/88 BatchLoss: G:0.749,D:1.404\n",
      "Batch:74/88 BatchLoss: G:0.736,D:1.405\n",
      "Batch:75/88 BatchLoss: G:0.721,D:1.404\n",
      "Batch:76/88 BatchLoss: G:0.796,D:1.400\n",
      "Batch:77/88 BatchLoss: G:0.775,D:1.392\n",
      "Batch:78/88 BatchLoss: G:0.740,D:1.394\n",
      "Batch:79/88 BatchLoss: G:0.833,D:1.393\n",
      "Batch:80/88 BatchLoss: G:0.895,D:1.396\n",
      "Batch:81/88 BatchLoss: G:0.844,D:1.391\n",
      "Batch:82/88 BatchLoss: G:0.885,D:1.394\n",
      "Batch:83/88 BatchLoss: G:0.893,D:1.394\n",
      "Batch:84/88 BatchLoss: G:0.858,D:1.391\n",
      "Batch:85/88 BatchLoss: G:0.790,D:1.382\n",
      "Batch:86/88 BatchLoss: G:0.839,D:1.386\n",
      "Batch:87/88 BatchLoss: G:0.995,D:1.423\n",
      "EpochLoss:G:72.218994140625,D:123.18231964111328\n",
      "Batch:0/88 BatchLoss: G:0.851,D:1.392\n",
      "Batch:1/88 BatchLoss: G:0.814,D:1.388\n",
      "Batch:2/88 BatchLoss: G:0.778,D:1.388\n",
      "Batch:3/88 BatchLoss: G:0.817,D:1.387\n",
      "Batch:4/88 BatchLoss: G:0.746,D:1.391\n",
      "Batch:5/88 BatchLoss: G:0.774,D:1.389\n",
      "Batch:6/88 BatchLoss: G:0.733,D:1.396\n",
      "Batch:7/88 BatchLoss: G:0.788,D:1.392\n",
      "Batch:8/88 BatchLoss: G:0.847,D:1.384\n",
      "Batch:9/88 BatchLoss: G:0.843,D:1.387\n",
      "Batch:10/88 BatchLoss: G:0.858,D:1.385\n",
      "Batch:11/88 BatchLoss: G:0.848,D:1.396\n",
      "Batch:12/88 BatchLoss: G:0.801,D:1.394\n",
      "Batch:13/88 BatchLoss: G:0.796,D:1.400\n",
      "Batch:14/88 BatchLoss: G:0.734,D:1.399\n",
      "Batch:15/88 BatchLoss: G:0.847,D:1.399\n",
      "Batch:16/88 BatchLoss: G:0.759,D:1.397\n",
      "Batch:17/88 BatchLoss: G:0.734,D:1.397\n",
      "Batch:18/88 BatchLoss: G:0.740,D:1.399\n",
      "Batch:19/88 BatchLoss: G:0.717,D:1.409\n",
      "Batch:20/88 BatchLoss: G:0.856,D:1.397\n",
      "Batch:21/88 BatchLoss: G:0.809,D:1.392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:22/88 BatchLoss: G:0.822,D:1.394\n",
      "Batch:23/88 BatchLoss: G:0.863,D:1.392\n",
      "Batch:24/88 BatchLoss: G:0.950,D:1.412\n",
      "Batch:25/88 BatchLoss: G:0.875,D:1.399\n",
      "Batch:26/88 BatchLoss: G:0.880,D:1.410\n",
      "Batch:27/88 BatchLoss: G:0.801,D:1.394\n",
      "Batch:28/88 BatchLoss: G:0.841,D:1.397\n",
      "Batch:29/88 BatchLoss: G:0.714,D:1.400\n",
      "Batch:30/88 BatchLoss: G:0.818,D:1.390\n",
      "Batch:31/88 BatchLoss: G:0.721,D:1.395\n",
      "Batch:32/88 BatchLoss: G:0.757,D:1.390\n",
      "Batch:33/88 BatchLoss: G:0.809,D:1.388\n",
      "Batch:34/88 BatchLoss: G:0.797,D:1.390\n",
      "Batch:35/88 BatchLoss: G:0.803,D:1.387\n",
      "Batch:36/88 BatchLoss: G:0.828,D:1.389\n",
      "Batch:37/88 BatchLoss: G:0.823,D:1.379\n",
      "Batch:38/88 BatchLoss: G:0.852,D:1.390\n",
      "Batch:39/88 BatchLoss: G:0.911,D:1.398\n",
      "Batch:40/88 BatchLoss: G:0.875,D:1.395\n",
      "Batch:41/88 BatchLoss: G:0.805,D:1.383\n",
      "Batch:42/88 BatchLoss: G:0.796,D:1.375\n",
      "Batch:43/88 BatchLoss: G:0.795,D:1.389\n",
      "Batch:44/88 BatchLoss: G:0.831,D:1.393\n",
      "Batch:45/88 BatchLoss: G:0.780,D:1.390\n",
      "Batch:46/88 BatchLoss: G:0.745,D:1.393\n",
      "Batch:47/88 BatchLoss: G:0.765,D:1.399\n",
      "Batch:48/88 BatchLoss: G:0.791,D:1.397\n",
      "Batch:49/88 BatchLoss: G:0.806,D:1.390\n",
      "Batch:50/88 BatchLoss: G:0.766,D:1.396\n",
      "Batch:51/88 BatchLoss: G:0.783,D:1.392\n",
      "Batch:52/88 BatchLoss: G:0.807,D:1.394\n",
      "Batch:53/88 BatchLoss: G:0.780,D:1.410\n",
      "Batch:54/88 BatchLoss: G:0.829,D:1.383\n",
      "Batch:55/88 BatchLoss: G:0.888,D:1.398\n",
      "Batch:56/88 BatchLoss: G:0.852,D:1.400\n",
      "Batch:57/88 BatchLoss: G:0.808,D:1.389\n",
      "Batch:58/88 BatchLoss: G:0.862,D:1.390\n",
      "Batch:59/88 BatchLoss: G:0.779,D:1.396\n",
      "Batch:60/88 BatchLoss: G:0.764,D:1.391\n",
      "Batch:61/88 BatchLoss: G:0.766,D:1.397\n",
      "Batch:62/88 BatchLoss: G:0.795,D:1.392\n",
      "Batch:63/88 BatchLoss: G:0.780,D:1.402\n",
      "Batch:64/88 BatchLoss: G:0.833,D:1.403\n",
      "Batch:65/88 BatchLoss: G:0.801,D:1.393\n",
      "Batch:66/88 BatchLoss: G:0.897,D:1.414\n",
      "Batch:67/88 BatchLoss: G:0.827,D:1.405\n",
      "Batch:68/88 BatchLoss: G:0.873,D:1.401\n",
      "Batch:69/88 BatchLoss: G:0.827,D:1.395\n",
      "Batch:70/88 BatchLoss: G:0.818,D:1.397\n",
      "Batch:71/88 BatchLoss: G:0.760,D:1.397\n",
      "Batch:72/88 BatchLoss: G:0.795,D:1.392\n",
      "Batch:73/88 BatchLoss: G:0.752,D:1.398\n",
      "Batch:74/88 BatchLoss: G:0.767,D:1.391\n",
      "Batch:75/88 BatchLoss: G:0.809,D:1.389\n",
      "Batch:76/88 BatchLoss: G:0.735,D:1.402\n",
      "Batch:77/88 BatchLoss: G:0.909,D:1.391\n",
      "Batch:78/88 BatchLoss: G:0.937,D:1.403\n",
      "Batch:79/88 BatchLoss: G:0.835,D:1.386\n",
      "Batch:80/88 BatchLoss: G:0.812,D:1.382\n",
      "Batch:81/88 BatchLoss: G:0.884,D:1.397\n",
      "Batch:82/88 BatchLoss: G:0.823,D:1.390\n",
      "Batch:83/88 BatchLoss: G:0.795,D:1.388\n",
      "Batch:84/88 BatchLoss: G:0.781,D:1.391\n",
      "Batch:85/88 BatchLoss: G:0.689,D:1.405\n",
      "Batch:86/88 BatchLoss: G:0.744,D:1.397\n",
      "Batch:87/88 BatchLoss: G:0.809,D:1.393\n",
      "EpochLoss:G:71.1175765991211,D:122.68571472167969\n",
      "Batch:0/88 BatchLoss: G:0.821,D:1.391\n",
      "Batch:1/88 BatchLoss: G:0.857,D:1.388\n",
      "Batch:2/88 BatchLoss: G:0.840,D:1.378\n",
      "Batch:3/88 BatchLoss: G:0.889,D:1.387\n",
      "Batch:4/88 BatchLoss: G:0.860,D:1.375\n",
      "Batch:5/88 BatchLoss: G:0.825,D:1.384\n",
      "Batch:6/88 BatchLoss: G:0.873,D:1.398\n",
      "Batch:7/88 BatchLoss: G:0.814,D:1.382\n",
      "Batch:8/88 BatchLoss: G:0.817,D:1.381\n",
      "Batch:9/88 BatchLoss: G:0.788,D:1.384\n",
      "Batch:10/88 BatchLoss: G:0.761,D:1.387\n",
      "Batch:11/88 BatchLoss: G:0.815,D:1.376\n",
      "Batch:12/88 BatchLoss: G:0.795,D:1.382\n",
      "Batch:13/88 BatchLoss: G:0.785,D:1.385\n",
      "Batch:14/88 BatchLoss: G:0.736,D:1.399\n",
      "Batch:15/88 BatchLoss: G:0.760,D:1.394\n",
      "Batch:16/88 BatchLoss: G:0.787,D:1.401\n",
      "Batch:17/88 BatchLoss: G:0.806,D:1.386\n",
      "Batch:18/88 BatchLoss: G:0.836,D:1.395\n",
      "Batch:19/88 BatchLoss: G:0.827,D:1.395\n",
      "Batch:20/88 BatchLoss: G:0.781,D:1.388\n",
      "Batch:21/88 BatchLoss: G:0.779,D:1.387\n",
      "Batch:22/88 BatchLoss: G:0.821,D:1.385\n",
      "Batch:23/88 BatchLoss: G:0.936,D:1.402\n",
      "Batch:24/88 BatchLoss: G:0.855,D:1.388\n",
      "Batch:25/88 BatchLoss: G:0.854,D:1.391\n",
      "Batch:26/88 BatchLoss: G:0.872,D:1.389\n",
      "Batch:27/88 BatchLoss: G:0.757,D:1.393\n",
      "Batch:28/88 BatchLoss: G:0.824,D:1.387\n",
      "Batch:29/88 BatchLoss: G:0.799,D:1.383\n",
      "Batch:30/88 BatchLoss: G:0.846,D:1.375\n",
      "Batch:31/88 BatchLoss: G:0.812,D:1.390\n",
      "Batch:32/88 BatchLoss: G:0.836,D:1.399\n",
      "Batch:33/88 BatchLoss: G:0.760,D:1.385\n",
      "Batch:34/88 BatchLoss: G:0.777,D:1.384\n",
      "Batch:35/88 BatchLoss: G:0.752,D:1.395\n",
      "Batch:36/88 BatchLoss: G:0.827,D:1.382\n",
      "Batch:37/88 BatchLoss: G:0.803,D:1.403\n",
      "Batch:38/88 BatchLoss: G:0.782,D:1.392\n",
      "Batch:39/88 BatchLoss: G:0.796,D:1.387\n",
      "Batch:40/88 BatchLoss: G:0.777,D:1.389\n",
      "Batch:41/88 BatchLoss: G:0.838,D:1.392\n",
      "Batch:42/88 BatchLoss: G:0.822,D:1.389\n",
      "Batch:43/88 BatchLoss: G:0.765,D:1.392\n",
      "Batch:44/88 BatchLoss: G:0.821,D:1.390\n",
      "Batch:45/88 BatchLoss: G:0.852,D:1.391\n",
      "Batch:46/88 BatchLoss: G:0.846,D:1.388\n",
      "Batch:47/88 BatchLoss: G:0.835,D:1.392\n",
      "Batch:48/88 BatchLoss: G:0.762,D:1.388\n",
      "Batch:49/88 BatchLoss: G:0.815,D:1.394\n",
      "Batch:50/88 BatchLoss: G:0.808,D:1.391\n",
      "Batch:51/88 BatchLoss: G:0.851,D:1.389\n",
      "Batch:52/88 BatchLoss: G:0.879,D:1.396\n",
      "Batch:53/88 BatchLoss: G:0.772,D:1.388\n",
      "Batch:54/88 BatchLoss: G:0.737,D:1.388\n",
      "Batch:55/88 BatchLoss: G:0.817,D:1.395\n",
      "Batch:56/88 BatchLoss: G:0.784,D:1.385\n",
      "Batch:57/88 BatchLoss: G:0.816,D:1.388\n",
      "Batch:58/88 BatchLoss: G:0.855,D:1.390\n",
      "Batch:59/88 BatchLoss: G:0.832,D:1.394\n",
      "Batch:60/88 BatchLoss: G:0.817,D:1.391\n",
      "Batch:61/88 BatchLoss: G:0.795,D:1.394\n",
      "Batch:62/88 BatchLoss: G:0.754,D:1.388\n",
      "Batch:63/88 BatchLoss: G:0.782,D:1.389\n",
      "Batch:64/88 BatchLoss: G:0.827,D:1.389\n",
      "Batch:65/88 BatchLoss: G:0.879,D:1.385\n",
      "Batch:66/88 BatchLoss: G:0.842,D:1.386\n",
      "Batch:67/88 BatchLoss: G:0.818,D:1.388\n",
      "Batch:68/88 BatchLoss: G:0.771,D:1.383\n",
      "Batch:69/88 BatchLoss: G:0.776,D:1.387\n",
      "Batch:70/88 BatchLoss: G:0.796,D:1.388\n",
      "Batch:71/88 BatchLoss: G:0.810,D:1.391\n",
      "Batch:72/88 BatchLoss: G:0.758,D:1.392\n",
      "Batch:73/88 BatchLoss: G:0.844,D:1.387\n",
      "Batch:74/88 BatchLoss: G:0.881,D:1.389\n",
      "Batch:75/88 BatchLoss: G:0.932,D:1.402\n",
      "Batch:76/88 BatchLoss: G:0.896,D:1.408\n",
      "Batch:77/88 BatchLoss: G:0.822,D:1.394\n",
      "Batch:78/88 BatchLoss: G:0.776,D:1.391\n",
      "Batch:79/88 BatchLoss: G:0.701,D:1.397\n",
      "Batch:80/88 BatchLoss: G:0.702,D:1.406\n",
      "Batch:81/88 BatchLoss: G:0.765,D:1.399\n",
      "Batch:82/88 BatchLoss: G:0.851,D:1.388\n",
      "Batch:83/88 BatchLoss: G:0.849,D:1.387\n",
      "Batch:84/88 BatchLoss: G:0.884,D:1.389\n",
      "Batch:85/88 BatchLoss: G:0.878,D:1.387\n",
      "Batch:86/88 BatchLoss: G:0.864,D:1.399\n",
      "Batch:87/88 BatchLoss: G:0.741,D:1.376\n",
      "EpochLoss:G:71.65132141113281,D:122.29469299316406\n",
      "Batch:0/88 BatchLoss: G:0.796,D:1.388\n",
      "Batch:1/88 BatchLoss: G:0.819,D:1.389\n",
      "Batch:2/88 BatchLoss: G:0.824,D:1.388\n",
      "Batch:3/88 BatchLoss: G:0.817,D:1.396\n",
      "Batch:4/88 BatchLoss: G:0.753,D:1.381\n",
      "Batch:5/88 BatchLoss: G:0.814,D:1.390\n",
      "Batch:6/88 BatchLoss: G:0.798,D:1.387\n",
      "Batch:7/88 BatchLoss: G:0.831,D:1.384\n",
      "Batch:8/88 BatchLoss: G:0.795,D:1.387\n",
      "Batch:9/88 BatchLoss: G:0.836,D:1.389\n",
      "Batch:10/88 BatchLoss: G:0.841,D:1.380\n",
      "Batch:11/88 BatchLoss: G:0.839,D:1.386\n",
      "Batch:12/88 BatchLoss: G:0.808,D:1.389\n",
      "Batch:13/88 BatchLoss: G:0.828,D:1.380\n",
      "Batch:14/88 BatchLoss: G:0.814,D:1.392\n",
      "Batch:15/88 BatchLoss: G:0.813,D:1.394\n",
      "Batch:16/88 BatchLoss: G:0.766,D:1.386\n",
      "Batch:17/88 BatchLoss: G:0.740,D:1.390\n",
      "Batch:18/88 BatchLoss: G:0.773,D:1.398\n",
      "Batch:19/88 BatchLoss: G:0.806,D:1.385\n",
      "Batch:20/88 BatchLoss: G:0.817,D:1.389\n",
      "Batch:21/88 BatchLoss: G:0.782,D:1.377\n",
      "Batch:22/88 BatchLoss: G:0.791,D:1.389\n",
      "Batch:23/88 BatchLoss: G:0.887,D:1.403\n",
      "Batch:24/88 BatchLoss: G:0.820,D:1.394\n",
      "Batch:25/88 BatchLoss: G:0.792,D:1.377\n",
      "Batch:26/88 BatchLoss: G:0.794,D:1.396\n",
      "Batch:27/88 BatchLoss: G:0.732,D:1.390\n",
      "Batch:28/88 BatchLoss: G:0.777,D:1.386\n",
      "Batch:29/88 BatchLoss: G:0.832,D:1.384\n",
      "Batch:30/88 BatchLoss: G:0.868,D:1.393\n",
      "Batch:31/88 BatchLoss: G:0.840,D:1.391\n",
      "Batch:32/88 BatchLoss: G:0.779,D:1.383\n",
      "Batch:33/88 BatchLoss: G:0.795,D:1.384\n",
      "Batch:34/88 BatchLoss: G:0.833,D:1.385\n",
      "Batch:35/88 BatchLoss: G:0.850,D:1.385\n",
      "Batch:36/88 BatchLoss: G:0.779,D:1.387\n",
      "Batch:37/88 BatchLoss: G:0.795,D:1.382\n",
      "Batch:38/88 BatchLoss: G:0.833,D:1.388\n",
      "Batch:39/88 BatchLoss: G:0.794,D:1.383\n",
      "Batch:40/88 BatchLoss: G:0.783,D:1.382\n",
      "Batch:41/88 BatchLoss: G:0.821,D:1.386\n",
      "Batch:42/88 BatchLoss: G:0.850,D:1.375\n",
      "Batch:43/88 BatchLoss: G:0.824,D:1.384\n",
      "Batch:44/88 BatchLoss: G:0.808,D:1.383\n",
      "Batch:45/88 BatchLoss: G:0.821,D:1.387\n",
      "Batch:46/88 BatchLoss: G:0.777,D:1.384\n",
      "Batch:47/88 BatchLoss: G:0.749,D:1.384\n",
      "Batch:48/88 BatchLoss: G:0.766,D:1.384\n",
      "Batch:49/88 BatchLoss: G:0.793,D:1.393\n",
      "Batch:50/88 BatchLoss: G:0.819,D:1.381\n",
      "Batch:51/88 BatchLoss: G:0.856,D:1.389\n",
      "Batch:52/88 BatchLoss: G:0.833,D:1.392\n",
      "Batch:53/88 BatchLoss: G:0.852,D:1.397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:54/88 BatchLoss: G:0.765,D:1.390\n",
      "Batch:55/88 BatchLoss: G:0.777,D:1.392\n",
      "Batch:56/88 BatchLoss: G:0.825,D:1.391\n",
      "Batch:57/88 BatchLoss: G:0.834,D:1.389\n",
      "Batch:58/88 BatchLoss: G:0.884,D:1.394\n",
      "Batch:59/88 BatchLoss: G:0.859,D:1.392\n",
      "Batch:60/88 BatchLoss: G:0.763,D:1.380\n",
      "Batch:61/88 BatchLoss: G:0.731,D:1.383\n",
      "Batch:62/88 BatchLoss: G:0.785,D:1.379\n",
      "Batch:63/88 BatchLoss: G:0.774,D:1.385\n",
      "Batch:64/88 BatchLoss: G:0.820,D:1.385\n",
      "Batch:65/88 BatchLoss: G:0.900,D:1.386\n",
      "Batch:66/88 BatchLoss: G:0.851,D:1.383\n",
      "Batch:67/88 BatchLoss: G:0.816,D:1.385\n",
      "Batch:68/88 BatchLoss: G:0.789,D:1.382\n",
      "Batch:69/88 BatchLoss: G:0.759,D:1.389\n",
      "Batch:70/88 BatchLoss: G:0.791,D:1.387\n",
      "Batch:71/88 BatchLoss: G:0.860,D:1.392\n",
      "Batch:72/88 BatchLoss: G:0.814,D:1.388\n",
      "Batch:73/88 BatchLoss: G:0.808,D:1.390\n",
      "Batch:74/88 BatchLoss: G:0.814,D:1.386\n",
      "Batch:75/88 BatchLoss: G:0.829,D:1.392\n",
      "Batch:76/88 BatchLoss: G:0.817,D:1.392\n",
      "Batch:77/88 BatchLoss: G:0.762,D:1.390\n",
      "Batch:78/88 BatchLoss: G:0.825,D:1.389\n",
      "Batch:79/88 BatchLoss: G:0.784,D:1.384\n",
      "Batch:80/88 BatchLoss: G:0.766,D:1.389\n",
      "Batch:81/88 BatchLoss: G:0.854,D:1.391\n",
      "Batch:82/88 BatchLoss: G:0.812,D:1.389\n",
      "Batch:83/88 BatchLoss: G:0.808,D:1.386\n",
      "Batch:84/88 BatchLoss: G:0.826,D:1.387\n",
      "Batch:85/88 BatchLoss: G:0.796,D:1.382\n",
      "Batch:86/88 BatchLoss: G:0.772,D:1.389\n",
      "Batch:87/88 BatchLoss: G:0.794,D:1.385\n",
      "EpochLoss:G:71.11981964111328,D:122.07780456542969\n",
      "Batch:0/88 BatchLoss: G:0.835,D:1.389\n",
      "Batch:1/88 BatchLoss: G:0.824,D:1.385\n",
      "Batch:2/88 BatchLoss: G:0.800,D:1.388\n",
      "Batch:3/88 BatchLoss: G:0.791,D:1.387\n",
      "Batch:4/88 BatchLoss: G:0.814,D:1.381\n",
      "Batch:5/88 BatchLoss: G:0.831,D:1.382\n",
      "Batch:6/88 BatchLoss: G:0.850,D:1.388\n",
      "Batch:7/88 BatchLoss: G:0.827,D:1.386\n",
      "Batch:8/88 BatchLoss: G:0.763,D:1.384\n",
      "Batch:9/88 BatchLoss: G:0.799,D:1.380\n",
      "Batch:10/88 BatchLoss: G:0.812,D:1.384\n",
      "Batch:11/88 BatchLoss: G:0.815,D:1.381\n",
      "Batch:12/88 BatchLoss: G:0.818,D:1.380\n",
      "Batch:13/88 BatchLoss: G:0.842,D:1.387\n",
      "Batch:14/88 BatchLoss: G:0.839,D:1.386\n",
      "Batch:15/88 BatchLoss: G:0.759,D:1.375\n",
      "Batch:16/88 BatchLoss: G:0.798,D:1.381\n",
      "Batch:17/88 BatchLoss: G:0.853,D:1.387\n",
      "Batch:18/88 BatchLoss: G:0.827,D:1.378\n",
      "Batch:19/88 BatchLoss: G:0.807,D:1.384\n",
      "Batch:20/88 BatchLoss: G:0.796,D:1.387\n",
      "Batch:21/88 BatchLoss: G:0.787,D:1.384\n",
      "Batch:22/88 BatchLoss: G:0.815,D:1.385\n",
      "Batch:23/88 BatchLoss: G:0.781,D:1.385\n",
      "Batch:24/88 BatchLoss: G:0.784,D:1.382\n",
      "Batch:25/88 BatchLoss: G:0.794,D:1.388\n",
      "Batch:26/88 BatchLoss: G:0.858,D:1.381\n",
      "Batch:27/88 BatchLoss: G:0.872,D:1.391\n",
      "Batch:28/88 BatchLoss: G:0.801,D:1.385\n",
      "Batch:29/88 BatchLoss: G:0.776,D:1.385\n",
      "Batch:30/88 BatchLoss: G:0.761,D:1.388\n",
      "Batch:31/88 BatchLoss: G:0.795,D:1.396\n",
      "Batch:32/88 BatchLoss: G:0.856,D:1.384\n",
      "Batch:33/88 BatchLoss: G:0.862,D:1.383\n",
      "Batch:34/88 BatchLoss: G:0.853,D:1.389\n",
      "Batch:35/88 BatchLoss: G:0.778,D:1.399\n",
      "Batch:36/88 BatchLoss: G:0.693,D:1.381\n",
      "Batch:37/88 BatchLoss: G:0.697,D:1.406\n",
      "Batch:38/88 BatchLoss: G:0.784,D:1.401\n",
      "Batch:39/88 BatchLoss: G:0.909,D:1.382\n",
      "Batch:40/88 BatchLoss: G:0.905,D:1.396\n",
      "Batch:41/88 BatchLoss: G:0.840,D:1.406\n",
      "Batch:42/88 BatchLoss: G:0.762,D:1.387\n",
      "Batch:43/88 BatchLoss: G:0.737,D:1.392\n",
      "Batch:44/88 BatchLoss: G:0.746,D:1.396\n",
      "Batch:45/88 BatchLoss: G:0.771,D:1.395\n",
      "Batch:46/88 BatchLoss: G:0.782,D:1.393\n",
      "Batch:47/88 BatchLoss: G:0.840,D:1.388\n",
      "Batch:48/88 BatchLoss: G:0.838,D:1.384\n",
      "Batch:49/88 BatchLoss: G:0.872,D:1.390\n",
      "Batch:50/88 BatchLoss: G:0.856,D:1.388\n",
      "Batch:51/88 BatchLoss: G:0.853,D:1.395\n",
      "Batch:52/88 BatchLoss: G:0.832,D:1.394\n",
      "Batch:53/88 BatchLoss: G:0.769,D:1.387\n",
      "Batch:54/88 BatchLoss: G:0.704,D:1.395\n",
      "Batch:55/88 BatchLoss: G:0.728,D:1.393\n",
      "Batch:56/88 BatchLoss: G:0.805,D:1.389\n",
      "Batch:57/88 BatchLoss: G:0.777,D:1.388\n",
      "Batch:58/88 BatchLoss: G:0.808,D:1.387\n",
      "Batch:59/88 BatchLoss: G:0.849,D:1.386\n",
      "Batch:60/88 BatchLoss: G:0.853,D:1.389\n",
      "Batch:61/88 BatchLoss: G:0.831,D:1.387\n",
      "Batch:62/88 BatchLoss: G:0.823,D:1.388\n",
      "Batch:63/88 BatchLoss: G:0.811,D:1.387\n",
      "Batch:64/88 BatchLoss: G:0.790,D:1.387\n",
      "Batch:65/88 BatchLoss: G:0.779,D:1.388\n",
      "Batch:66/88 BatchLoss: G:0.780,D:1.388\n",
      "Batch:67/88 BatchLoss: G:0.765,D:1.388\n",
      "Batch:68/88 BatchLoss: G:0.782,D:1.390\n",
      "Batch:69/88 BatchLoss: G:0.796,D:1.386\n",
      "Batch:70/88 BatchLoss: G:0.804,D:1.386\n",
      "Batch:71/88 BatchLoss: G:0.803,D:1.386\n",
      "Batch:72/88 BatchLoss: G:0.827,D:1.385\n",
      "Batch:73/88 BatchLoss: G:0.855,D:1.392\n",
      "Batch:74/88 BatchLoss: G:0.845,D:1.391\n",
      "Batch:75/88 BatchLoss: G:0.775,D:1.386\n",
      "Batch:76/88 BatchLoss: G:0.772,D:1.385\n",
      "Batch:77/88 BatchLoss: G:0.800,D:1.386\n",
      "Batch:78/88 BatchLoss: G:0.793,D:1.385\n",
      "Batch:79/88 BatchLoss: G:0.801,D:1.385\n",
      "Batch:80/88 BatchLoss: G:0.824,D:1.386\n",
      "Batch:81/88 BatchLoss: G:0.824,D:1.389\n",
      "Batch:82/88 BatchLoss: G:0.786,D:1.380\n",
      "Batch:83/88 BatchLoss: G:0.755,D:1.388\n",
      "Batch:84/88 BatchLoss: G:0.782,D:1.382\n",
      "Batch:85/88 BatchLoss: G:0.781,D:1.386\n",
      "Batch:86/88 BatchLoss: G:0.826,D:1.381\n",
      "Batch:87/88 BatchLoss: G:0.823,D:1.376\n",
      "EpochLoss:G:70.911376953125,D:122.0772476196289\n",
      "Batch:0/88 BatchLoss: G:0.794,D:1.381\n",
      "Batch:1/88 BatchLoss: G:0.811,D:1.382\n",
      "Batch:2/88 BatchLoss: G:0.799,D:1.382\n",
      "Batch:3/88 BatchLoss: G:0.790,D:1.381\n",
      "Batch:4/88 BatchLoss: G:0.767,D:1.378\n",
      "Batch:5/88 BatchLoss: G:0.797,D:1.377\n",
      "Batch:6/88 BatchLoss: G:0.792,D:1.384\n",
      "Batch:7/88 BatchLoss: G:0.831,D:1.383\n",
      "Batch:8/88 BatchLoss: G:0.828,D:1.386\n",
      "Batch:9/88 BatchLoss: G:0.807,D:1.388\n",
      "Batch:10/88 BatchLoss: G:0.799,D:1.386\n",
      "Batch:11/88 BatchLoss: G:0.760,D:1.381\n",
      "Batch:12/88 BatchLoss: G:0.756,D:1.382\n",
      "Batch:13/88 BatchLoss: G:0.784,D:1.389\n",
      "Batch:14/88 BatchLoss: G:0.806,D:1.383\n",
      "Batch:15/88 BatchLoss: G:0.833,D:1.387\n",
      "Batch:16/88 BatchLoss: G:0.828,D:1.383\n",
      "Batch:17/88 BatchLoss: G:0.803,D:1.393\n",
      "Batch:18/88 BatchLoss: G:0.806,D:1.384\n",
      "Batch:19/88 BatchLoss: G:0.771,D:1.386\n",
      "Batch:20/88 BatchLoss: G:0.788,D:1.392\n",
      "Batch:21/88 BatchLoss: G:0.806,D:1.388\n",
      "Batch:22/88 BatchLoss: G:0.833,D:1.385\n",
      "Batch:23/88 BatchLoss: G:0.782,D:1.380\n",
      "Batch:24/88 BatchLoss: G:0.746,D:1.394\n",
      "Batch:25/88 BatchLoss: G:0.796,D:1.385\n",
      "Batch:26/88 BatchLoss: G:0.811,D:1.386\n",
      "Batch:27/88 BatchLoss: G:0.799,D:1.387\n",
      "Batch:28/88 BatchLoss: G:0.801,D:1.383\n",
      "Batch:29/88 BatchLoss: G:0.787,D:1.381\n",
      "Batch:30/88 BatchLoss: G:0.791,D:1.382\n",
      "Batch:31/88 BatchLoss: G:0.800,D:1.387\n",
      "Batch:32/88 BatchLoss: G:0.804,D:1.382\n",
      "Batch:33/88 BatchLoss: G:0.800,D:1.383\n",
      "Batch:34/88 BatchLoss: G:0.815,D:1.380\n",
      "Batch:35/88 BatchLoss: G:0.812,D:1.378\n",
      "Batch:36/88 BatchLoss: G:0.808,D:1.380\n",
      "Batch:37/88 BatchLoss: G:0.807,D:1.381\n",
      "Batch:38/88 BatchLoss: G:0.799,D:1.385\n",
      "Batch:39/88 BatchLoss: G:0.819,D:1.379\n",
      "Batch:40/88 BatchLoss: G:0.792,D:1.382\n",
      "Batch:41/88 BatchLoss: G:0.784,D:1.383\n",
      "Batch:42/88 BatchLoss: G:0.785,D:1.379\n",
      "Batch:43/88 BatchLoss: G:0.829,D:1.384\n",
      "Batch:44/88 BatchLoss: G:0.783,D:1.378\n",
      "Batch:45/88 BatchLoss: G:0.771,D:1.382\n",
      "Batch:46/88 BatchLoss: G:0.762,D:1.380\n",
      "Batch:47/88 BatchLoss: G:0.803,D:1.381\n",
      "Batch:48/88 BatchLoss: G:0.832,D:1.382\n",
      "Batch:49/88 BatchLoss: G:0.805,D:1.379\n",
      "Batch:50/88 BatchLoss: G:0.833,D:1.385\n",
      "Batch:51/88 BatchLoss: G:0.800,D:1.382\n",
      "Batch:52/88 BatchLoss: G:0.800,D:1.381\n",
      "Batch:53/88 BatchLoss: G:0.800,D:1.385\n",
      "Batch:54/88 BatchLoss: G:0.786,D:1.381\n",
      "Batch:55/88 BatchLoss: G:0.805,D:1.380\n",
      "Batch:56/88 BatchLoss: G:0.815,D:1.381\n",
      "Batch:57/88 BatchLoss: G:0.777,D:1.384\n",
      "Batch:58/88 BatchLoss: G:0.784,D:1.385\n",
      "Batch:59/88 BatchLoss: G:0.786,D:1.382\n",
      "Batch:60/88 BatchLoss: G:0.822,D:1.376\n",
      "Batch:61/88 BatchLoss: G:0.833,D:1.381\n",
      "Batch:62/88 BatchLoss: G:0.803,D:1.377\n",
      "Batch:63/88 BatchLoss: G:0.777,D:1.380\n",
      "Batch:64/88 BatchLoss: G:0.801,D:1.379\n",
      "Batch:65/88 BatchLoss: G:0.781,D:1.375\n",
      "Batch:66/88 BatchLoss: G:0.783,D:1.381\n",
      "Batch:67/88 BatchLoss: G:0.828,D:1.379\n",
      "Batch:68/88 BatchLoss: G:0.840,D:1.381\n",
      "Batch:69/88 BatchLoss: G:0.807,D:1.380\n",
      "Batch:70/88 BatchLoss: G:0.777,D:1.383\n",
      "Batch:71/88 BatchLoss: G:0.759,D:1.382\n",
      "Batch:72/88 BatchLoss: G:0.807,D:1.383\n",
      "Batch:73/88 BatchLoss: G:0.845,D:1.382\n",
      "Batch:74/88 BatchLoss: G:0.852,D:1.386\n",
      "Batch:75/88 BatchLoss: G:0.772,D:1.381\n",
      "Batch:76/88 BatchLoss: G:0.740,D:1.381\n",
      "Batch:77/88 BatchLoss: G:0.783,D:1.387\n",
      "Batch:78/88 BatchLoss: G:0.823,D:1.384\n",
      "Batch:79/88 BatchLoss: G:0.855,D:1.389\n",
      "Batch:80/88 BatchLoss: G:0.854,D:1.389\n",
      "Batch:81/88 BatchLoss: G:0.795,D:1.385\n",
      "Batch:82/88 BatchLoss: G:0.735,D:1.382\n",
      "Batch:83/88 BatchLoss: G:0.753,D:1.388\n",
      "Batch:84/88 BatchLoss: G:0.803,D:1.386\n",
      "Batch:85/88 BatchLoss: G:0.826,D:1.387\n",
      "Batch:86/88 BatchLoss: G:0.786,D:1.386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:87/88 BatchLoss: G:0.788,D:1.384\n",
      "EpochLoss:G:70.32579803466797,D:121.70934295654297\n",
      "Batch:0/88 BatchLoss: G:0.802,D:1.383\n",
      "Batch:1/88 BatchLoss: G:0.796,D:1.384\n",
      "Batch:2/88 BatchLoss: G:0.831,D:1.389\n",
      "Batch:3/88 BatchLoss: G:0.816,D:1.389\n",
      "Batch:4/88 BatchLoss: G:0.802,D:1.385\n",
      "Batch:5/88 BatchLoss: G:0.794,D:1.383\n",
      "Batch:6/88 BatchLoss: G:0.787,D:1.380\n",
      "Batch:7/88 BatchLoss: G:0.791,D:1.384\n",
      "Batch:8/88 BatchLoss: G:0.811,D:1.381\n",
      "Batch:9/88 BatchLoss: G:0.822,D:1.382\n",
      "Batch:10/88 BatchLoss: G:0.784,D:1.378\n",
      "Batch:11/88 BatchLoss: G:0.772,D:1.381\n",
      "Batch:12/88 BatchLoss: G:0.770,D:1.381\n",
      "Batch:13/88 BatchLoss: G:0.790,D:1.381\n",
      "Batch:14/88 BatchLoss: G:0.813,D:1.380\n",
      "Batch:15/88 BatchLoss: G:0.815,D:1.379\n",
      "Batch:16/88 BatchLoss: G:0.793,D:1.377\n",
      "Batch:17/88 BatchLoss: G:0.775,D:1.378\n",
      "Batch:18/88 BatchLoss: G:0.800,D:1.382\n",
      "Batch:19/88 BatchLoss: G:0.816,D:1.381\n",
      "Batch:20/88 BatchLoss: G:0.813,D:1.384\n",
      "Batch:21/88 BatchLoss: G:0.814,D:1.382\n",
      "Batch:22/88 BatchLoss: G:0.786,D:1.380\n",
      "Batch:23/88 BatchLoss: G:0.769,D:1.381\n",
      "Batch:24/88 BatchLoss: G:0.781,D:1.383\n",
      "Batch:25/88 BatchLoss: G:0.809,D:1.384\n",
      "Batch:26/88 BatchLoss: G:0.801,D:1.380\n",
      "Batch:27/88 BatchLoss: G:0.806,D:1.380\n",
      "Batch:28/88 BatchLoss: G:0.793,D:1.379\n",
      "Batch:29/88 BatchLoss: G:0.803,D:1.385\n",
      "Batch:30/88 BatchLoss: G:0.804,D:1.381\n",
      "Batch:31/88 BatchLoss: G:0.789,D:1.379\n",
      "Batch:32/88 BatchLoss: G:0.795,D:1.379\n",
      "Batch:33/88 BatchLoss: G:0.790,D:1.379\n",
      "Batch:34/88 BatchLoss: G:0.804,D:1.382\n",
      "Batch:35/88 BatchLoss: G:0.795,D:1.378\n",
      "Batch:36/88 BatchLoss: G:0.808,D:1.378\n",
      "Batch:37/88 BatchLoss: G:0.812,D:1.375\n",
      "Batch:38/88 BatchLoss: G:0.823,D:1.383\n",
      "Batch:39/88 BatchLoss: G:0.805,D:1.382\n",
      "Batch:40/88 BatchLoss: G:0.758,D:1.378\n",
      "Batch:41/88 BatchLoss: G:0.748,D:1.381\n",
      "Batch:42/88 BatchLoss: G:0.795,D:1.378\n",
      "Batch:43/88 BatchLoss: G:0.829,D:1.381\n",
      "Batch:44/88 BatchLoss: G:0.844,D:1.384\n",
      "Batch:45/88 BatchLoss: G:0.814,D:1.385\n",
      "Batch:46/88 BatchLoss: G:0.789,D:1.379\n",
      "Batch:47/88 BatchLoss: G:0.764,D:1.377\n",
      "Batch:48/88 BatchLoss: G:0.804,D:1.381\n",
      "Batch:49/88 BatchLoss: G:0.829,D:1.374\n",
      "Batch:50/88 BatchLoss: G:0.817,D:1.381\n",
      "Batch:51/88 BatchLoss: G:0.796,D:1.382\n",
      "Batch:52/88 BatchLoss: G:0.773,D:1.376\n",
      "Batch:53/88 BatchLoss: G:0.758,D:1.380\n",
      "Batch:54/88 BatchLoss: G:0.810,D:1.384\n",
      "Batch:55/88 BatchLoss: G:0.832,D:1.384\n",
      "Batch:56/88 BatchLoss: G:0.810,D:1.377\n",
      "Batch:57/88 BatchLoss: G:0.799,D:1.384\n",
      "Batch:58/88 BatchLoss: G:0.774,D:1.381\n",
      "Batch:59/88 BatchLoss: G:0.809,D:1.376\n",
      "Batch:60/88 BatchLoss: G:0.803,D:1.382\n",
      "Batch:61/88 BatchLoss: G:0.818,D:1.382\n",
      "Batch:62/88 BatchLoss: G:0.772,D:1.379\n",
      "Batch:63/88 BatchLoss: G:0.774,D:1.373\n",
      "Batch:64/88 BatchLoss: G:0.812,D:1.378\n",
      "Batch:65/88 BatchLoss: G:0.846,D:1.381\n",
      "Batch:66/88 BatchLoss: G:0.824,D:1.384\n",
      "Batch:67/88 BatchLoss: G:0.780,D:1.383\n",
      "Batch:68/88 BatchLoss: G:0.733,D:1.381\n",
      "Batch:69/88 BatchLoss: G:0.778,D:1.382\n",
      "Batch:70/88 BatchLoss: G:0.831,D:1.380\n",
      "Batch:71/88 BatchLoss: G:0.859,D:1.385\n",
      "Batch:72/88 BatchLoss: G:0.830,D:1.387\n",
      "Batch:73/88 BatchLoss: G:0.760,D:1.382\n",
      "Batch:74/88 BatchLoss: G:0.731,D:1.381\n",
      "Batch:75/88 BatchLoss: G:0.760,D:1.385\n",
      "Batch:76/88 BatchLoss: G:0.812,D:1.383\n",
      "Batch:77/88 BatchLoss: G:0.854,D:1.380\n",
      "Batch:78/88 BatchLoss: G:0.829,D:1.383\n",
      "Batch:79/88 BatchLoss: G:0.788,D:1.381\n",
      "Batch:80/88 BatchLoss: G:0.770,D:1.381\n",
      "Batch:81/88 BatchLoss: G:0.742,D:1.384\n",
      "Batch:82/88 BatchLoss: G:0.802,D:1.381\n",
      "Batch:83/88 BatchLoss: G:0.867,D:1.381\n",
      "Batch:84/88 BatchLoss: G:0.849,D:1.383\n",
      "Batch:85/88 BatchLoss: G:0.805,D:1.383\n",
      "Batch:86/88 BatchLoss: G:0.777,D:1.378\n",
      "Batch:87/88 BatchLoss: G:0.774,D:1.381\n",
      "EpochLoss:G:70.3133773803711,D:121.54106140136719\n",
      "Batch:0/88 BatchLoss: G:0.803,D:1.377\n",
      "Batch:1/88 BatchLoss: G:0.820,D:1.374\n",
      "Batch:2/88 BatchLoss: G:0.790,D:1.374\n",
      "Batch:3/88 BatchLoss: G:0.775,D:1.382\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f8bf1bbf495c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mGTrainLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mG_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mGTrainLoss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0mG_opt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mEpochGTrainLoss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mGTrainLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gpxtensor/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/gpxtensor/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 初始化\n",
    "TrainSetPath = ''\n",
    "lr=0.0005\n",
    "BatchSize = 10\n",
    "Epochs = 100\n",
    "\n",
    "#指定某块GPU进行训练\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2'\n",
    "\n",
    "# 加载数据\n",
    "TrainLoader=DataLoader(dataset=TrainData,batch_size=BatchSize,shuffle=True)\n",
    "\n",
    "\n",
    "# 训练模型\n",
    "G = HDRGen()\n",
    "D = HDRDis()\n",
    "\n",
    "G.cuda()\n",
    "D.cuda()\n",
    "\n",
    "if os.path.exists('GenModel.pkl'):\n",
    "    G.load_state_dict(torch.load('GenModel.pkl'))\n",
    "    print('G model load successfully')\n",
    "    \n",
    "if os.path.exists('DisModel.pkl'):\n",
    "    D.load_state_dict(torch.load('DisModel.pkl'))\n",
    "    print('D model load successfully') \n",
    "    \n",
    "G_opt = torch.optim.Adam(G.parameters(),lr=lr,betas=(0.9,0.99),weight_decay=0.0005)\n",
    "D_opt = torch.optim.Adam(D.parameters(),lr=lr,betas=(0.9,0.99),weight_decay=0.0005)\n",
    "# 设置损失函数\n",
    "Gloss =G_loss()\n",
    "Dloss = D_loss()\n",
    "\n",
    "# 设置学习率下降策略\n",
    "G_sch = torch.optim.lr_scheduler.StepLR(G_opt, step_size=1, gamma=0.98)  \n",
    "D_sch = torch.optim.lr_scheduler.StepLR(D_opt, step_size=1, gamma=0.98)  \n",
    "\n",
    "G.train()\n",
    "D.train()\n",
    "\n",
    "for epoch in range(Epochs):\n",
    "    #更新学习率\n",
    "    D_sch.step()\n",
    "    G_sch.step()\n",
    "    EpochTrainCorrect = 0\n",
    "    EpochGTrainLoss = 0\n",
    "    EpochDTrainLoss = 0\n",
    "    for batch, data in enumerate(TrainLoader):\n",
    "        DTrainLoss = Dloss(D,G,data[0].cuda(),data[1].cuda())\n",
    "        D_opt.zero_grad()\n",
    "        DTrainLoss.backward()\n",
    "        D_opt.step()\n",
    "        EpochDTrainLoss+=DTrainLoss\n",
    "        \n",
    "        GTrainLoss = Gloss(G,data[0].cuda(),data[1].cuda())\n",
    "        G_opt.zero_grad()\n",
    "        GTrainLoss.backward()\n",
    "        G_opt.step()\n",
    "        EpochGTrainLoss+=GTrainLoss\n",
    "        \n",
    "        print('Batch:{}/{} BatchLoss: G:{:.3f},D:{:.3f}'.format(batch,len(TrainLoader),GTrainLoss,DTrainLoss))\n",
    "    print('EpochLoss:G:{},D:{}'.format(EpochGTrainLoss,EpochDTrainLoss))\n",
    "    torch.save(G.state_dict(),'GenModel.pkl')\n",
    "    torch.save(D.state_dict(),'DisModel.pkl')\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
